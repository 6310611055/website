{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn import preprocessing\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>afftype</th>\n",
       "      <th>melanch</th>\n",
       "      <th>inpatient</th>\n",
       "      <th>edu</th>\n",
       "      <th>marriage</th>\n",
       "      <th>work</th>\n",
       "      <th>madrs1</th>\n",
       "      <th>5days_sleep_time_activity</th>\n",
       "      <th>5days_day_time_activity</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>10693.6</td>\n",
       "      <td>228824.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>48771.2</td>\n",
       "      <td>239278.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>43211.0</td>\n",
       "      <td>317726.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>56892.4</td>\n",
       "      <td>194298.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>31303.8</td>\n",
       "      <td>200302.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>26634.2</td>\n",
       "      <td>240767.8</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>61643.6</td>\n",
       "      <td>335598.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>34374.2</td>\n",
       "      <td>284320.6</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>42992.0</td>\n",
       "      <td>203120.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>25811.0</td>\n",
       "      <td>482765.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>50147.0</td>\n",
       "      <td>153494.6</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>34576.6</td>\n",
       "      <td>228420.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>62330.8</td>\n",
       "      <td>291661.6</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>42878.0</td>\n",
       "      <td>72237.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>42016.2</td>\n",
       "      <td>164387.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>17938.8</td>\n",
       "      <td>397321.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>24344.6</td>\n",
       "      <td>85835.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>40038.4</td>\n",
       "      <td>64142.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>17014.0</td>\n",
       "      <td>189703.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>10301.8</td>\n",
       "      <td>84496.8</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>4627.0</td>\n",
       "      <td>91568.8</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>18023.6</td>\n",
       "      <td>183974.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>29581.2</td>\n",
       "      <td>235831.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>62703.4</td>\n",
       "      <td>257546.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>51929.0</td>\n",
       "      <td>475113.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>34742.8</td>\n",
       "      <td>347817.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23481.0</td>\n",
       "      <td>255676.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>49904.8</td>\n",
       "      <td>398723.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>103205.0</td>\n",
       "      <td>431101.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>115329.4</td>\n",
       "      <td>479807.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>124525.0</td>\n",
       "      <td>467588.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19149.8</td>\n",
       "      <td>169119.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>35635.0</td>\n",
       "      <td>293293.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>37543.8</td>\n",
       "      <td>233625.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>30825.2</td>\n",
       "      <td>227922.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>28298.2</td>\n",
       "      <td>246814.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>97899.4</td>\n",
       "      <td>471340.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>64710.4</td>\n",
       "      <td>373257.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>50966.2</td>\n",
       "      <td>343819.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>54870.4</td>\n",
       "      <td>304521.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>54821.6</td>\n",
       "      <td>327886.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>79226.0</td>\n",
       "      <td>233218.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>62504.0</td>\n",
       "      <td>480728.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>129507.8</td>\n",
       "      <td>260240.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>41799.6</td>\n",
       "      <td>326000.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>43280.4</td>\n",
       "      <td>249467.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65133.6</td>\n",
       "      <td>584654.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>67783.8</td>\n",
       "      <td>462165.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>111668.2</td>\n",
       "      <td>464377.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>84647.2</td>\n",
       "      <td>331799.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>96948.4</td>\n",
       "      <td>343388.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60045.6</td>\n",
       "      <td>327889.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>45442.2</td>\n",
       "      <td>284416.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60903.2</td>\n",
       "      <td>392232.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>47928.8</td>\n",
       "      <td>151957.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender  age  afftype  melanch  inpatient  edu  marriage  work  madrs1  \\\n",
       "0        2   37        2        2          2    8         1     2      19   \n",
       "1        2   42        1        2          2    8         2     2      24   \n",
       "2        1   47        2        2          2    8         2     2      24   \n",
       "3        2   27        2        2          2   13         1     1      20   \n",
       "4        2   52        2        2          2   13         2     2      26   \n",
       "5        1   37        2        2          2    8         1     2      18   \n",
       "6        1   22        1        2          2   13         2     1      24   \n",
       "7        2   27        2        2          2   13         1     2      20   \n",
       "8        2   47        1        2          2    8         1     2      26   \n",
       "9        2   47        2        2          2    8         1     2      28   \n",
       "10       1   47        2        2          2    8         1     2      24   \n",
       "11       2   42        1        2          2    8         2     2      25   \n",
       "12       2   37        1        2          2   13         2     2      18   \n",
       "13       1   62        1        2          2    8         2     2      28   \n",
       "14       2   57        2        2          2   13         1     1      14   \n",
       "15       1   47        2        2          2   13         1     2      13   \n",
       "16       1   52        1        2          2    8         1     2      17   \n",
       "17       2   42        3        2          2   13         2     2      18   \n",
       "18       2   52        2        2          1   18         2     2      26   \n",
       "19       1   32        2        1          1    8         1     2      27   \n",
       "20       2   37        2        2          1    8         2     2      26   \n",
       "21       1   67        2        2          1    8         2     2      29   \n",
       "22       1   32        2        2          1   18         2     2      29   \n",
       "23       2   27        0        2          0   13         2     1       4   \n",
       "24       1   32        0        2          0    8         1     1       4   \n",
       "25       2   32        0        2          0   18         1     1       4   \n",
       "26       1   27        0        2          0   18         2     2       4   \n",
       "27       1   32        0        2          0    8         1     1       4   \n",
       "28       1   27        0        2          0   13         2     1       9   \n",
       "29       1   22        0        2          0   13         2     1       4   \n",
       "30       2   42        0        2          0   18         1     1       4   \n",
       "31       2   32        0        2          0   18         1     1       4   \n",
       "32       1   32        0        2          0    8         1     1       4   \n",
       "33       1   47        0        2          0    8         1     1       4   \n",
       "34       1   62        0        2          0   13         1     1       4   \n",
       "35       1   52        0        2          0    8         1     1       4   \n",
       "36       1   52        0        2          0    8         1     1       4   \n",
       "37       1   47        0        2          0    8         1     1       4   \n",
       "38       2   42        0        2          0   18         1     1       4   \n",
       "39       1   47        0        2          0    8         1     1       4   \n",
       "40       2   22        0        2          0   13         2     1       4   \n",
       "41       1   52        0        2          0    8         1     1       4   \n",
       "42       1   37        0        2          0    8         1     1       9   \n",
       "43       1   52        0        2          0    8         1     1       4   \n",
       "44       1   27        0        2          0   18         1     1       4   \n",
       "45       1   22        0        2          0   13         2     1       4   \n",
       "46       2   22        0        2          0    8         2     1       4   \n",
       "47       1   67        0        2          0   13         1     1       4   \n",
       "48       1   37        0        2          0    8         1     1       4   \n",
       "49       2   52        0        2          0    8         2     1       4   \n",
       "50       2   47        0        2          0    8         2     1       4   \n",
       "51       2   52        0        2          0   13         2     1       4   \n",
       "52       2   37        0        2          0    8         1     1       4   \n",
       "53       1   22        2        2          0    8         2     1       4   \n",
       "54       2   27        0        2          0   13         2     1       4   \n",
       "\n",
       "    5days_sleep_time_activity  5days_day_time_activity         Id  \n",
       "0                     10693.6                 228824.0  condition  \n",
       "1                     48771.2                 239278.2  condition  \n",
       "2                     43211.0                 317726.0  condition  \n",
       "3                     56892.4                 194298.2  condition  \n",
       "4                     31303.8                 200302.2  condition  \n",
       "5                     26634.2                 240767.8  condition  \n",
       "6                     61643.6                 335598.2  condition  \n",
       "7                     34374.2                 284320.6  condition  \n",
       "8                     42992.0                 203120.0  condition  \n",
       "9                     25811.0                 482765.0  condition  \n",
       "10                    50147.0                 153494.6  condition  \n",
       "11                    34576.6                 228420.4  condition  \n",
       "12                    62330.8                 291661.6  condition  \n",
       "13                    42878.0                  72237.4  condition  \n",
       "14                    42016.2                 164387.4  condition  \n",
       "15                    17938.8                 397321.0  condition  \n",
       "16                    24344.6                  85835.4  condition  \n",
       "17                    40038.4                  64142.0  condition  \n",
       "18                    17014.0                 189703.4  condition  \n",
       "19                    10301.8                  84496.8  condition  \n",
       "20                     4627.0                  91568.8  condition  \n",
       "21                    18023.6                 183974.4  condition  \n",
       "22                    29581.2                 235831.4  condition  \n",
       "23                    62703.4                 257546.4    control  \n",
       "24                    51929.0                 475113.4    control  \n",
       "25                    34742.8                 347817.0    control  \n",
       "26                    23481.0                 255676.4    control  \n",
       "27                    49904.8                 398723.0    control  \n",
       "28                   103205.0                 431101.8    control  \n",
       "29                   115329.4                 479807.0    control  \n",
       "30                   124525.0                 467588.6    control  \n",
       "31                    19149.8                 169119.2    control  \n",
       "32                    35635.0                 293293.2    control  \n",
       "33                    37543.8                 233625.8    control  \n",
       "34                    30825.2                 227922.0    control  \n",
       "35                    28298.2                 246814.8    control  \n",
       "36                    97899.4                 471340.6    control  \n",
       "37                    64710.4                 373257.6    control  \n",
       "38                    50966.2                 343819.2    control  \n",
       "39                    54870.4                 304521.0    control  \n",
       "40                    54821.6                 327886.0    control  \n",
       "41                    79226.0                 233218.6    control  \n",
       "42                    62504.0                 480728.2    control  \n",
       "43                   129507.8                 260240.4    control  \n",
       "44                    41799.6                 326000.6    control  \n",
       "45                    43280.4                 249467.8    control  \n",
       "46                    65133.6                 584654.0    control  \n",
       "47                    67783.8                 462165.4    control  \n",
       "48                   111668.2                 464377.6    control  \n",
       "49                    84647.2                 331799.6    control  \n",
       "50                    96948.4                 343388.2    control  \n",
       "51                    60045.6                 327889.8    control  \n",
       "52                    45442.2                 284416.2    control  \n",
       "53                    60903.2                 392232.0    control  \n",
       "54                    47928.8                 151957.4    control  "
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df = pd.read_csv(\n",
    "    \"../Dataset/dataset_filled_missing.csv\")\n",
    "ml_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "ml_df['Id'] = ['condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'control', 'control',\n",
    "               'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control']\n",
    "ml_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select: 55 set\n",
      "[[1.069360e+04 2.288240e+05 3.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.900000e+01\n",
      "  1.000000e+00]\n",
      " [4.877120e+04 2.392782e+05 4.200000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [4.321100e+04 3.177260e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [5.689240e+04 1.942982e+05 2.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 2.000000e+01\n",
      "  1.000000e+00]\n",
      " [3.130380e+04 2.003022e+05 5.200000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [2.663420e+04 2.407678e+05 3.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.800000e+01\n",
      "  1.000000e+00]\n",
      " [6.164360e+04 3.355982e+05 2.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [3.437420e+04 2.843206e+05 2.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 2.000000e+01\n",
      "  1.000000e+00]\n",
      " [4.299200e+04 2.031200e+05 4.700000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.600000e+01\n",
      "  1.000000e+00]\n",
      " [2.581100e+04 4.827650e+05 4.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.800000e+01\n",
      "  1.000000e+00]\n",
      " [5.014700e+04 1.534946e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  1.000000e+00]\n",
      " [3.457660e+04 2.284204e+05 4.200000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.500000e+01\n",
      "  2.000000e+00]\n",
      " [6.233080e+04 2.916616e+05 3.700000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.800000e+01\n",
      "  2.000000e+00]\n",
      " [4.287800e+04 7.223740e+04 6.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.800000e+01\n",
      "  2.000000e+00]\n",
      " [4.201620e+04 1.643874e+05 5.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 1.400000e+01\n",
      "  1.000000e+00]\n",
      " [1.793880e+04 3.973210e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.300000e+01\n",
      "  1.000000e+00]\n",
      " [2.434460e+04 8.583540e+04 5.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.700000e+01\n",
      "  1.000000e+00]\n",
      " [4.003840e+04 6.414200e+04 4.200000e+01 2.000000e+00 3.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.800000e+01\n",
      "  2.000000e+00]\n",
      " [1.701400e+04 1.897034e+05 5.200000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 1.800000e+01 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [1.030180e+04 8.449680e+04 3.200000e+01 1.000000e+00 2.000000e+00\n",
      "  1.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.700000e+01\n",
      "  1.000000e+00]\n",
      " [4.627000e+03 9.156880e+04 3.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [1.802360e+04 1.839744e+05 6.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.900000e+01\n",
      "  2.000000e+00]\n",
      " [2.958120e+04 2.358314e+05 3.200000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 1.800000e+01 2.000000e+00 2.900000e+01\n",
      "  2.000000e+00]\n",
      " [6.270340e+04 2.575464e+05 2.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [5.192900e+04 4.751134e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.474280e+04 3.478170e+05 3.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.348100e+04 2.556764e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 2.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [4.990480e+04 3.987230e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.032050e+05 4.311018e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 9.000000e+00\n",
      "  2.000000e+00]\n",
      " [1.153294e+05 4.798070e+05 2.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [1.245250e+05 4.675886e+05 4.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.914980e+04 1.691192e+05 3.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.563500e+04 2.932932e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.754380e+04 2.336258e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.082520e+04 2.279220e+05 6.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.829820e+04 2.468148e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [9.789940e+04 4.713406e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.471040e+04 3.732576e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [5.096620e+04 3.438192e+05 4.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [5.487040e+04 3.045210e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [5.482160e+04 3.278860e+05 2.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [7.922600e+04 2.332186e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.250400e+04 4.807282e+05 3.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 9.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.295078e+05 2.602404e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.179960e+04 3.260006e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.328040e+04 2.494678e+05 2.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [6.513360e+04 5.846540e+05 2.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [6.778380e+04 4.621654e+05 6.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.116682e+05 4.643776e+05 3.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [8.464720e+04 3.317996e+05 5.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [9.694840e+04 3.433882e+05 4.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [6.004560e+04 3.278898e+05 5.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [4.544220e+04 2.844162e+05 3.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.090320e+04 3.922320e+05 2.200000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [4.792880e+04 1.519574e+05 2.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "count = 0\n",
    "count_g = 0\n",
    "count_o = 0\n",
    "\n",
    "for i in range(len(ml_df[\"Id\"])):\n",
    "\n",
    "    if ml_df[\"Id\"][i] == \"condition\":\n",
    "        x.append([1])\n",
    "        y.append([ml_df[\"5days_sleep_time_activity\"][i],\n",
    "                  ml_df[\"5days_day_time_activity\"][i], ml_df[\"age\"][i], ml_df[\"gender\"][i], ml_df[\"afftype\"][i],\n",
    "                  ml_df[\"melanch\"][i], ml_df[\"inpatient\"][i], ml_df[\"edu\"][i], ml_df[\"work\"][i], ml_df[\"madrs1\"][i], ml_df[\"marriage\"][i]])\n",
    "\n",
    "    elif ml_df[\"Id\"][i] == \"control\":\n",
    "        x.append([0])\n",
    "        y.append([ml_df[\"5days_sleep_time_activity\"][i],\n",
    "                  ml_df[\"5days_day_time_activity\"][i], ml_df[\"age\"][i], ml_df[\"gender\"][i], ml_df[\"afftype\"][i],\n",
    "                  ml_df[\"melanch\"][i], ml_df[\"inpatient\"][i], ml_df[\"edu\"][i], ml_df[\"work\"][i], ml_df[\"madrs1\"][i], ml_df[\"marriage\"][i]])\n",
    "\n",
    "\n",
    "print(f'Select: {len(y)} set')\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "print(y)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "# Train Test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[5.014700e+04 1.534946e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  1.000000e+00]\n",
      " [1.032050e+05 4.311018e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 9.000000e+00\n",
      "  2.000000e+00]\n",
      " [2.958120e+04 2.358314e+05 3.200000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 1.800000e+01 2.000000e+00 2.900000e+01\n",
      "  2.000000e+00]\n",
      " [1.914980e+04 1.691192e+05 3.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [8.464720e+04 3.317996e+05 5.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [6.471040e+04 3.732576e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.437420e+04 2.843206e+05 2.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 2.000000e+01\n",
      "  1.000000e+00]\n",
      " [4.201620e+04 1.643874e+05 5.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 1.400000e+01\n",
      "  1.000000e+00]\n",
      " [4.990480e+04 3.987230e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.829820e+04 2.468148e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [9.694840e+04 3.433882e+05 4.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [1.701400e+04 1.897034e+05 5.200000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 1.800000e+01 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [4.544220e+04 2.844162e+05 3.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.082520e+04 2.279220e+05 6.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.793880e+04 3.973210e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.300000e+01\n",
      "  1.000000e+00]\n",
      " [2.663420e+04 2.407678e+05 3.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.800000e+01\n",
      "  1.000000e+00]\n",
      " [1.153294e+05 4.798070e+05 2.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [2.434460e+04 8.583540e+04 5.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.700000e+01\n",
      "  1.000000e+00]\n",
      " [6.090320e+04 3.922320e+05 2.200000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [4.627000e+03 9.156880e+04 3.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [1.116682e+05 4.643776e+05 3.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.299200e+04 2.031200e+05 4.700000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.600000e+01\n",
      "  1.000000e+00]\n",
      " [4.287800e+04 7.223740e+04 6.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.800000e+01\n",
      "  2.000000e+00]\n",
      " [3.474280e+04 3.478170e+05 3.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.003840e+04 6.414200e+04 4.200000e+01 2.000000e+00 3.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.800000e+01\n",
      "  2.000000e+00]\n",
      " [7.922600e+04 2.332186e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.792880e+04 1.519574e+05 2.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [5.096620e+04 3.438192e+05 4.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.877120e+04 2.392782e+05 4.200000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [6.233080e+04 2.916616e+05 3.700000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.800000e+01\n",
      "  2.000000e+00]\n",
      " [6.250400e+04 4.807282e+05 3.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 9.000000e+00\n",
      "  1.000000e+00]\n",
      " [5.192900e+04 4.751134e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.164360e+04 3.355982e+05 2.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [6.270340e+04 2.575464e+05 2.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [9.789940e+04 4.713406e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.802360e+04 1.839744e+05 6.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.900000e+01\n",
      "  2.000000e+00]\n",
      " [1.030180e+04 8.449680e+04 3.200000e+01 1.000000e+00 2.000000e+00\n",
      "  1.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.700000e+01\n",
      "  1.000000e+00]\n",
      " [2.581100e+04 4.827650e+05 4.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.800000e+01\n",
      "  1.000000e+00]\n",
      " [5.487040e+04 3.045210e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.004560e+04 3.278898e+05 5.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [5.689240e+04 1.942982e+05 2.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 2.000000e+01\n",
      "  1.000000e+00]\n",
      " [1.069360e+04 2.288240e+05 3.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.900000e+01\n",
      "  1.000000e+00]\n",
      " [6.778380e+04 4.621654e+05 6.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.179960e+04 3.260006e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_modeling():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(70, input_shape=(11,), activation='relu'))\n",
    "    model.add(Dense(65, activation='relu'))\n",
    "    model.add(Dense(125, activation='relu'))\n",
    "    # model.add(Dense(140, activation='relu'))\n",
    "    # model.add(Dense(70, activation='relu'))\n",
    "    # model.add(Dense(20, activation='elu'))\n",
    "    # model.add(Dense(130, activation='softsign'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing model\n",
    "# model = Sequential()\n",
    "# model.add(Dense(115, input_shape=(11,), activation='sigmoid'))\n",
    "# model.add(Dense(55, activation='sigmoid'))\n",
    "# model.add(Dense(85, activation='sigmoid'))\n",
    "# model.add(Dense(2, activation='selu'))\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='Adamax', metrics=['accuracy'])\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# le.fit(x_train)\n",
    "# x_train = le.transform(x_train)\n",
    "# x_train = keras.utils.to_categorical(x_train)\n",
    "\n",
    "# model.fit(y_train, x_train, validation_split=0.2, epochs=50, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 678ms/step - loss: 5627.9341 - accuracy: 0.3929 - val_loss: 2194.2593 - val_accuracy: 0.4286\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1166.1293 - accuracy: 0.5357 - val_loss: 5266.3271 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3291.8254 - accuracy: 0.5714 - val_loss: 6689.0659 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4111.8369 - accuracy: 0.6071 - val_loss: 6103.1162 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3416.3931 - accuracy: 0.6071 - val_loss: 4335.7422 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1877.8549 - accuracy: 0.6429 - val_loss: 3641.1199 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1109.5684 - accuracy: 0.6429 - val_loss: 2835.3496 - val_accuracy: 0.2857\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1768.7804 - accuracy: 0.5357 - val_loss: 2428.7986 - val_accuracy: 0.7143\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2372.3013 - accuracy: 0.4286 - val_loss: 1441.2418 - val_accuracy: 0.8571\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2257.0144 - accuracy: 0.3929 - val_loss: 285.5771 - val_accuracy: 0.8571\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1958.2402 - accuracy: 0.3929 - val_loss: 287.6992 - val_accuracy: 0.8571\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1530.2463 - accuracy: 0.3929 - val_loss: 704.8004 - val_accuracy: 0.7143\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 545.6207 - accuracy: 0.5000 - val_loss: 3161.8445 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 861.9753 - accuracy: 0.6429 - val_loss: 4590.9419 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1390.8479 - accuracy: 0.6429 - val_loss: 4416.0425 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1324.6178 - accuracy: 0.6429 - val_loss: 3742.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1488.9520 - accuracy: 0.6429 - val_loss: 2899.7778 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1359.7791 - accuracy: 0.6429 - val_loss: 1679.2531 - val_accuracy: 0.5714\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 808.6805 - accuracy: 0.5000 - val_loss: 669.8810 - val_accuracy: 0.7143\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 403.8315 - accuracy: 0.5000 - val_loss: 1424.6747 - val_accuracy: 0.7143\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 964.9219 - accuracy: 0.5000 - val_loss: 1966.5651 - val_accuracy: 0.7143\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1296.4886 - accuracy: 0.3929 - val_loss: 2006.1027 - val_accuracy: 0.5714\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1190.0596 - accuracy: 0.5000 - val_loss: 1674.6158 - val_accuracy: 0.1429\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 719.3115 - accuracy: 0.5357 - val_loss: 1226.5570 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 229.6067 - accuracy: 0.5714 - val_loss: 1769.6107 - val_accuracy: 0.5714\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 921.7835 - accuracy: 0.5000 - val_loss: 2061.6021 - val_accuracy: 0.7143\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1306.8512 - accuracy: 0.5000 - val_loss: 1838.9186 - val_accuracy: 0.7143\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1276.0765 - accuracy: 0.4643 - val_loss: 1172.1295 - val_accuracy: 0.7143\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 883.5784 - accuracy: 0.4286 - val_loss: 455.3593 - val_accuracy: 0.7143\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 471.0754 - accuracy: 0.4286 - val_loss: 1022.6142 - val_accuracy: 0.7143\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 506.2093 - accuracy: 0.5000 - val_loss: 1787.1879 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 770.3115 - accuracy: 0.6429 - val_loss: 2129.0859 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 697.0001 - accuracy: 0.6429 - val_loss: 2209.1213 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 588.3828 - accuracy: 0.6429 - val_loss: 1339.2921 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 281.1580 - accuracy: 0.6429 - val_loss: 356.0169 - val_accuracy: 0.8571\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 436.9225 - accuracy: 0.3929 - val_loss: 288.8705 - val_accuracy: 0.8571\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 670.3246 - accuracy: 0.3929 - val_loss: 631.4525 - val_accuracy: 0.8571\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 653.6169 - accuracy: 0.3929 - val_loss: 703.3802 - val_accuracy: 0.7143\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 452.3677 - accuracy: 0.4643 - val_loss: 519.4465 - val_accuracy: 0.2857\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 108.1881 - accuracy: 0.6071 - val_loss: 1365.0964 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 374.3617 - accuracy: 0.6429 - val_loss: 1952.1825 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 563.6216 - accuracy: 0.6429 - val_loss: 1384.7676 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 386.2873 - accuracy: 0.6429 - val_loss: 703.3080 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 111.0734 - accuracy: 0.7500 - val_loss: 293.6774 - val_accuracy: 0.8571\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 234.9443 - accuracy: 0.4286 - val_loss: 700.1537 - val_accuracy: 0.7143\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 436.2058 - accuracy: 0.4286 - val_loss: 721.6227 - val_accuracy: 0.7143\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 327.7333 - accuracy: 0.5000 - val_loss: 537.1927 - val_accuracy: 0.1429\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 79.6199 - accuracy: 0.6071 - val_loss: 1093.7999 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 249.6499 - accuracy: 0.6429 - val_loss: 1294.4716 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 287.4900 - accuracy: 0.6429 - val_loss: 517.9660 - val_accuracy: 0.7143\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 141.1381 - accuracy: 0.5000 - val_loss: 299.7182 - val_accuracy: 0.7143\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 227.2853 - accuracy: 0.3929 - val_loss: 957.1939 - val_accuracy: 0.1429\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 396.9692 - accuracy: 0.6429 - val_loss: 1265.0953 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 303.7792 - accuracy: 0.6429 - val_loss: 1513.5175 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 349.5813 - accuracy: 0.6429 - val_loss: 952.9824 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 312.1938 - accuracy: 0.7143 - val_loss: 484.6413 - val_accuracy: 0.7143\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 268.8461 - accuracy: 0.4286 - val_loss: 295.6333 - val_accuracy: 0.8571\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 344.2376 - accuracy: 0.3929 - val_loss: 793.4673 - val_accuracy: 0.7143\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 330.4074 - accuracy: 0.5000 - val_loss: 1179.4760 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 274.8007 - accuracy: 0.6429 - val_loss: 1697.4384 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 398.4550 - accuracy: 0.6429 - val_loss: 1260.6171 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 360.2679 - accuracy: 0.6429 - val_loss: 910.8402 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 326.3938 - accuracy: 0.6786 - val_loss: 361.5692 - val_accuracy: 0.7143\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 155.2922 - accuracy: 0.4286 - val_loss: 998.3890 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 316.2282 - accuracy: 0.7500 - val_loss: 1416.6770 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 309.5575 - accuracy: 0.6429 - val_loss: 1228.0287 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 220.6720 - accuracy: 0.6429 - val_loss: 387.1222 - val_accuracy: 0.7143\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 94.6224 - accuracy: 0.4643 - val_loss: 642.4053 - val_accuracy: 0.1429\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 163.3407 - accuracy: 0.5357 - val_loss: 593.3558 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 33.2518 - accuracy: 0.6786 - val_loss: 674.6115 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 117.1268 - accuracy: 0.7143 - val_loss: 631.5684 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 29.5423 - accuracy: 0.7143 - val_loss: 631.7421 - val_accuracy: 0.2857\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 132.7346 - accuracy: 0.5000 - val_loss: 416.5071 - val_accuracy: 0.7143\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 101.2335 - accuracy: 0.4286 - val_loss: 1292.7375 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 233.1068 - accuracy: 0.6429 - val_loss: 1785.2578 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 384.8929 - accuracy: 0.6429 - val_loss: 1109.3744 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 162.4963 - accuracy: 0.6429 - val_loss: 421.8514 - val_accuracy: 0.7143\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 189.9938 - accuracy: 0.4286 - val_loss: 696.1578 - val_accuracy: 0.7143\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 244.0898 - accuracy: 0.4286 - val_loss: 615.9821 - val_accuracy: 0.2857\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 82.0240 - accuracy: 0.5000 - val_loss: 1157.6422 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 217.5816 - accuracy: 0.6429 - val_loss: 1551.4431 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 300.8187 - accuracy: 0.6429 - val_loss: 1182.0294 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 200.1557 - accuracy: 0.6429 - val_loss: 569.1117 - val_accuracy: 0.2857\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 58.7131 - accuracy: 0.5357 - val_loss: 612.4775 - val_accuracy: 0.7143\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 126.7302 - accuracy: 0.4643 - val_loss: 638.5196 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 17.9476 - accuracy: 0.7857 - val_loss: 745.1748 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 40.6002 - accuracy: 0.7500 - val_loss: 636.9891 - val_accuracy: 0.2857\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 98.1470 - accuracy: 0.5357 - val_loss: 600.0142 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 43.9590 - accuracy: 0.6071 - val_loss: 1106.7970 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 160.0979 - accuracy: 0.6429 - val_loss: 1166.2919 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 171.9718 - accuracy: 0.6429 - val_loss: 775.1840 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 20.2266 - accuracy: 0.7500 - val_loss: 351.5154 - val_accuracy: 0.8571\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 372.9932 - accuracy: 0.3929 - val_loss: 381.3958 - val_accuracy: 0.8571\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 328.9375 - accuracy: 0.3929 - val_loss: 1061.1726 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 207.9183 - accuracy: 0.6429 - val_loss: 2072.2043 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 486.4468 - accuracy: 0.6429 - val_loss: 1920.0208 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 426.5065 - accuracy: 0.6429 - val_loss: 1147.7511 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 485.6154 - accuracy: 0.7500 - val_loss: 376.8947 - val_accuracy: 0.8571\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 394.5355 - accuracy: 0.3929 - val_loss: 297.4802 - val_accuracy: 0.8571\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 429.4646 - accuracy: 0.3929 - val_loss: 577.2730 - val_accuracy: 0.2857\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 69.6357 - accuracy: 0.5357 - val_loss: 1701.8417 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 340.4014 - accuracy: 0.6429 - val_loss: 1692.7316 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 333.7016 - accuracy: 0.6429 - val_loss: 612.8248 - val_accuracy: 0.1429\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 29.8360 - accuracy: 0.6071 - val_loss: 380.2619 - val_accuracy: 0.7143\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 302.5121 - accuracy: 0.3929 - val_loss: 532.7007 - val_accuracy: 0.7143\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 181.9664 - accuracy: 0.4286 - val_loss: 1023.5384 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 85.5594 - accuracy: 0.6071 - val_loss: 988.6740 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 111.1194 - accuracy: 0.7143 - val_loss: 686.7360 - val_accuracy: 0.1429\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 48.7130 - accuracy: 0.6429 - val_loss: 807.8789 - val_accuracy: 0.1429\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 173.1425 - accuracy: 0.6071 - val_loss: 688.4917 - val_accuracy: 0.4286\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 96.2473 - accuracy: 0.5357 - val_loss: 910.4488 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 169.3614 - accuracy: 0.7857 - val_loss: 992.8372 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 123.2260 - accuracy: 0.7143 - val_loss: 1090.0198 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 159.4976 - accuracy: 0.6429 - val_loss: 1064.7688 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 151.4451 - accuracy: 0.6429 - val_loss: 941.7117 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 109.1608 - accuracy: 0.7143 - val_loss: 814.6704 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 85.1363 - accuracy: 0.7857 - val_loss: 818.4609 - val_accuracy: 0.2857\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 183.9399 - accuracy: 0.5714 - val_loss: 690.0319 - val_accuracy: 0.7143\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 162.2547 - accuracy: 0.4643 - val_loss: 699.1952 - val_accuracy: 0.1429\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 62.1231 - accuracy: 0.5714 - val_loss: 1092.4943 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 119.2028 - accuracy: 0.6429 - val_loss: 724.7656 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 25.3686 - accuracy: 0.7143 - val_loss: 678.3517 - val_accuracy: 0.7143\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 159.7602 - accuracy: 0.4643 - val_loss: 672.4664 - val_accuracy: 0.7143\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 115.9349 - accuracy: 0.5000 - val_loss: 902.9547 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 113.7482 - accuracy: 0.7857 - val_loss: 1132.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 97.8833 - accuracy: 0.6429 - val_loss: 606.0052 - val_accuracy: 0.7143\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 84.0938 - accuracy: 0.5000 - val_loss: 942.3029 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 100.0560 - accuracy: 0.7500 - val_loss: 1205.2737 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 118.6221 - accuracy: 0.6429 - val_loss: 913.9948 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 163.8602 - accuracy: 0.7143 - val_loss: 556.1354 - val_accuracy: 0.7143\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 135.9013 - accuracy: 0.4286 - val_loss: 1150.7206 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 222.5026 - accuracy: 0.6429 - val_loss: 1631.9698 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 273.7146 - accuracy: 0.6429 - val_loss: 1048.0326 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 58.6764 - accuracy: 0.6071 - val_loss: 353.8767 - val_accuracy: 0.8571\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 416.1278 - accuracy: 0.3929 - val_loss: 332.7693 - val_accuracy: 0.8571\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 479.1967 - accuracy: 0.3929 - val_loss: 659.8600 - val_accuracy: 0.7143\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 159.5341 - accuracy: 0.4643 - val_loss: 1645.6100 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 273.6283 - accuracy: 0.6429 - val_loss: 1657.4171 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 277.5152 - accuracy: 0.6429 - val_loss: 970.0326 - val_accuracy: 0.1429\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 300.6822 - accuracy: 0.5714 - val_loss: 383.8166 - val_accuracy: 0.8571\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 378.6710 - accuracy: 0.3929 - val_loss: 406.4251 - val_accuracy: 0.7143\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 271.3484 - accuracy: 0.3929 - val_loss: 1290.2843 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 202.5394 - accuracy: 0.6071 - val_loss: 1928.9880 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 376.5483 - accuracy: 0.6429 - val_loss: 1468.6927 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 237.6210 - accuracy: 0.6429 - val_loss: 532.2279 - val_accuracy: 0.7143\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 150.3506 - accuracy: 0.4643 - val_loss: 471.9768 - val_accuracy: 0.7143\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 204.2987 - accuracy: 0.4286 - val_loss: 1072.9623 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 137.9057 - accuracy: 0.7500 - val_loss: 1706.1180 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 282.2859 - accuracy: 0.6429 - val_loss: 1240.0514 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 140.9174 - accuracy: 0.6071 - val_loss: 493.4767 - val_accuracy: 0.7143\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 860ms/step - loss: 1480.0609 - accuracy: 0.4286 - val_loss: 8379.0654 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2336.2703 - accuracy: 0.6786 - val_loss: 7493.4185 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2248.4312 - accuracy: 0.6786 - val_loss: 4783.6406 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1655.1562 - accuracy: 0.6786 - val_loss: 1660.6652 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 574.3160 - accuracy: 0.5714 - val_loss: 1190.8035 - val_accuracy: 0.8571\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1360.6368 - accuracy: 0.3929 - val_loss: 620.6895 - val_accuracy: 1.0000\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1720.5441 - accuracy: 0.3929 - val_loss: 255.7654 - val_accuracy: 1.0000\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1475.0342 - accuracy: 0.3929 - val_loss: 741.8474 - val_accuracy: 0.8571\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 862.4727 - accuracy: 0.3929 - val_loss: 1060.0745 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 265.2717 - accuracy: 0.6786 - val_loss: 1844.6536 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 846.1813 - accuracy: 0.6786 - val_loss: 2071.6138 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 909.5858 - accuracy: 0.6786 - val_loss: 1651.8367 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 572.6349 - accuracy: 0.6786 - val_loss: 884.3945 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 258.6072 - accuracy: 0.6786 - val_loss: 504.8262 - val_accuracy: 0.8571\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 663.4277 - accuracy: 0.4286 - val_loss: 78.8801 - val_accuracy: 0.8571\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 864.6844 - accuracy: 0.3929 - val_loss: 16.4670 - val_accuracy: 0.8571\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 894.1484 - accuracy: 0.3929 - val_loss: 160.0776 - val_accuracy: 0.8571\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 404.7856 - accuracy: 0.3929 - val_loss: 1404.4808 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 353.5240 - accuracy: 0.6786 - val_loss: 2274.7258 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 619.6359 - accuracy: 0.6786 - val_loss: 1950.8333 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 515.6027 - accuracy: 0.6786 - val_loss: 1186.9353 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 585.7769 - accuracy: 0.6786 - val_loss: 293.1562 - val_accuracy: 0.8571\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 394.2685 - accuracy: 0.3929 - val_loss: 179.4006 - val_accuracy: 0.8571\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 515.8958 - accuracy: 0.3929 - val_loss: 634.7933 - val_accuracy: 0.8571\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 518.0347 - accuracy: 0.4643 - val_loss: 1093.7968 - val_accuracy: 0.4286\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 482.3332 - accuracy: 0.5000 - val_loss: 1332.1766 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 310.0921 - accuracy: 0.6786 - val_loss: 1360.4323 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 310.4975 - accuracy: 0.6786 - val_loss: 949.7641 - val_accuracy: 0.4286\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 450.5930 - accuracy: 0.5714 - val_loss: 427.8825 - val_accuracy: 0.8571\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 413.1352 - accuracy: 0.5000 - val_loss: 214.5201 - val_accuracy: 0.8571\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 430.6641 - accuracy: 0.3929 - val_loss: 324.3039 - val_accuracy: 0.8571\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 259.2408 - accuracy: 0.5000 - val_loss: 1066.4338 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 304.1381 - accuracy: 0.7500 - val_loss: 1829.9598 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 413.7907 - accuracy: 0.6786 - val_loss: 1870.9280 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 405.6829 - accuracy: 0.6786 - val_loss: 998.3055 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 132.1776 - accuracy: 0.6786 - val_loss: 296.9362 - val_accuracy: 0.7143\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 335.7678 - accuracy: 0.3929 - val_loss: 238.4282 - val_accuracy: 0.8571\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 489.3648 - accuracy: 0.3929 - val_loss: 377.5966 - val_accuracy: 0.7143\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 228.5459 - accuracy: 0.4286 - val_loss: 1416.2369 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 246.9896 - accuracy: 0.6786 - val_loss: 1935.2882 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 404.2661 - accuracy: 0.6786 - val_loss: 1535.3533 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 277.9543 - accuracy: 0.6786 - val_loss: 534.8369 - val_accuracy: 0.7143\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 78.6972 - accuracy: 0.6071 - val_loss: 305.5882 - val_accuracy: 0.7143\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 297.9744 - accuracy: 0.3929 - val_loss: 343.2358 - val_accuracy: 0.7143\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 224.0057 - accuracy: 0.5000 - val_loss: 917.5190 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 115.4126 - accuracy: 0.6786 - val_loss: 1317.1184 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 223.2894 - accuracy: 0.6786 - val_loss: 829.7316 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 71.5600 - accuracy: 0.6786 - val_loss: 318.4719 - val_accuracy: 0.7143\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 212.7946 - accuracy: 0.4643 - val_loss: 313.9184 - val_accuracy: 0.7143\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 226.3318 - accuracy: 0.4286 - val_loss: 647.4349 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 37.5610 - accuracy: 0.7500 - val_loss: 1032.2869 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 122.1756 - accuracy: 0.6786 - val_loss: 603.8839 - val_accuracy: 0.1429\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 34.0069 - accuracy: 0.7857 - val_loss: 510.2733 - val_accuracy: 0.7143\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 103.1165 - accuracy: 0.6071 - val_loss: 654.0394 - val_accuracy: 0.2857\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 88.4154 - accuracy: 0.6429 - val_loss: 885.8602 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 152.2791 - accuracy: 0.8571 - val_loss: 1020.6132 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 177.2299 - accuracy: 0.6786 - val_loss: 1059.3242 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 113.7744 - accuracy: 0.6786 - val_loss: 642.5777 - val_accuracy: 0.7143\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 157.6842 - accuracy: 0.6071 - val_loss: 347.9240 - val_accuracy: 0.7143\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 250.6077 - accuracy: 0.4286 - val_loss: 400.8113 - val_accuracy: 0.7143\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 138.0107 - accuracy: 0.5357 - val_loss: 1130.9961 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 169.9167 - accuracy: 0.6786 - val_loss: 1912.4226 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 391.6364 - accuracy: 0.6786 - val_loss: 1787.1031 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 351.5027 - accuracy: 0.6786 - val_loss: 838.1422 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 55.6350 - accuracy: 0.6429 - val_loss: 251.1856 - val_accuracy: 0.8571\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 389.0351 - accuracy: 0.3929 - val_loss: 214.8066 - val_accuracy: 0.8571\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 515.5973 - accuracy: 0.3929 - val_loss: 313.8933 - val_accuracy: 0.7143\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 229.6468 - accuracy: 0.5000 - val_loss: 1208.7399 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 172.5238 - accuracy: 0.6786 - val_loss: 1737.9797 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 337.8216 - accuracy: 0.6786 - val_loss: 1391.7821 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 226.0028 - accuracy: 0.6786 - val_loss: 565.3649 - val_accuracy: 0.7143\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 119.5056 - accuracy: 0.5714 - val_loss: 305.8299 - val_accuracy: 0.7143\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 322.0060 - accuracy: 0.3929 - val_loss: 358.1166 - val_accuracy: 0.7143\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 297.8937 - accuracy: 0.4286 - val_loss: 773.9456 - val_accuracy: 0.1429\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 155.1536 - accuracy: 0.7500 - val_loss: 1327.4728 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 185.8432 - accuracy: 0.6786 - val_loss: 1265.5842 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 223.9108 - accuracy: 0.6786 - val_loss: 1154.1517 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 186.1450 - accuracy: 0.6786 - val_loss: 1027.7764 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 102.3753 - accuracy: 0.6786 - val_loss: 651.4107 - val_accuracy: 0.7143\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 194.3760 - accuracy: 0.5714 - val_loss: 334.1408 - val_accuracy: 0.7143\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 282.9825 - accuracy: 0.3929 - val_loss: 374.6014 - val_accuracy: 0.7143\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 192.7766 - accuracy: 0.5000 - val_loss: 960.7021 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 135.0365 - accuracy: 0.7143 - val_loss: 1857.3308 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 354.8662 - accuracy: 0.6786 - val_loss: 1905.6432 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 370.4224 - accuracy: 0.6786 - val_loss: 1145.5723 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 166.7739 - accuracy: 0.6786 - val_loss: 407.1642 - val_accuracy: 0.7143\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 128.3374 - accuracy: 0.5357 - val_loss: 364.6608 - val_accuracy: 0.7143\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 179.7146 - accuracy: 0.5357 - val_loss: 625.5015 - val_accuracy: 0.1429\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 63.1341 - accuracy: 0.7143 - val_loss: 1252.2615 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 160.0287 - accuracy: 0.6786 - val_loss: 1125.8497 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 177.5949 - accuracy: 0.6786 - val_loss: 770.7850 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 52.6593 - accuracy: 0.7500 - val_loss: 631.3911 - val_accuracy: 0.7143\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 212.3469 - accuracy: 0.5357 - val_loss: 465.8021 - val_accuracy: 0.7143\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 224.9248 - accuracy: 0.5357 - val_loss: 421.4175 - val_accuracy: 0.7143\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 110.3974 - accuracy: 0.5357 - val_loss: 1127.2501 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 168.6860 - accuracy: 0.6786 - val_loss: 1702.8368 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 301.0642 - accuracy: 0.6786 - val_loss: 1424.5079 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 215.3344 - accuracy: 0.6786 - val_loss: 496.7424 - val_accuracy: 0.7143\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 63.9795 - accuracy: 0.5714 - val_loss: 355.9073 - val_accuracy: 0.7143\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 201.0547 - accuracy: 0.5357 - val_loss: 471.5361 - val_accuracy: 0.7143\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 140.5309 - accuracy: 0.5357 - val_loss: 928.0421 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 103.2032 - accuracy: 0.7500 - val_loss: 1582.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 247.6463 - accuracy: 0.6786 - val_loss: 1420.4244 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 258.9039 - accuracy: 0.6786 - val_loss: 1088.3129 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 188.6746 - accuracy: 0.7143 - val_loss: 595.6112 - val_accuracy: 0.5714\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 57.4749 - accuracy: 0.6429 - val_loss: 786.2059 - val_accuracy: 0.5714\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 203.2280 - accuracy: 0.6071 - val_loss: 700.3999 - val_accuracy: 0.2857\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 86.0416 - accuracy: 0.6786 - val_loss: 895.6538 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 181.5415 - accuracy: 0.8214 - val_loss: 975.3647 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 206.3212 - accuracy: 0.8214 - val_loss: 794.3315 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 18.1818 - accuracy: 0.7500 - val_loss: 783.4860 - val_accuracy: 0.2857\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 176.5747 - accuracy: 0.6786 - val_loss: 595.3746 - val_accuracy: 0.7143\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 127.8580 - accuracy: 0.5714 - val_loss: 518.5441 - val_accuracy: 0.7143\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 78.0990 - accuracy: 0.5714 - val_loss: 1078.0405 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 204.4193 - accuracy: 0.7143 - val_loss: 1307.2917 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 149.7764 - accuracy: 0.6786 - val_loss: 1004.6116 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 54.6755 - accuracy: 0.6786 - val_loss: 516.5891 - val_accuracy: 0.7143\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 149.3260 - accuracy: 0.5357 - val_loss: 438.9844 - val_accuracy: 0.7143\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 167.2845 - accuracy: 0.5357 - val_loss: 655.9947 - val_accuracy: 0.1429\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.0762 - accuracy: 0.7857 - val_loss: 1540.0228 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 230.8015 - accuracy: 0.6786 - val_loss: 1528.4856 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 231.0343 - accuracy: 0.6786 - val_loss: 862.8260 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 155.9738 - accuracy: 0.8929 - val_loss: 322.4253 - val_accuracy: 0.7143\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 214.6377 - accuracy: 0.5000 - val_loss: 371.4995 - val_accuracy: 0.7143\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 217.3338 - accuracy: 0.5357 - val_loss: 887.4283 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 211.0863 - accuracy: 0.8214 - val_loss: 1426.5559 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 202.6652 - accuracy: 0.6786 - val_loss: 1313.0107 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 166.1280 - accuracy: 0.6786 - val_loss: 660.9655 - val_accuracy: 0.4286\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 149.4336 - accuracy: 0.5714 - val_loss: 277.4685 - val_accuracy: 0.8571\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 280.9303 - accuracy: 0.4286 - val_loss: 321.7188 - val_accuracy: 0.7143\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 219.6418 - accuracy: 0.5000 - val_loss: 936.9078 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 177.2189 - accuracy: 0.7500 - val_loss: 1777.1819 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 303.3389 - accuracy: 0.6786 - val_loss: 1836.5216 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 317.4581 - accuracy: 0.6786 - val_loss: 1073.0469 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 113.0460 - accuracy: 0.6786 - val_loss: 404.8674 - val_accuracy: 0.7143\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 143.3872 - accuracy: 0.5357 - val_loss: 368.6913 - val_accuracy: 0.7143\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 204.7709 - accuracy: 0.5000 - val_loss: 601.5735 - val_accuracy: 0.2857\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 55.1068 - accuracy: 0.6786 - val_loss: 1321.0768 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 145.0788 - accuracy: 0.6786 - val_loss: 1255.0692 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 131.4012 - accuracy: 0.6786 - val_loss: 753.6027 - val_accuracy: 0.1429\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 62.6836 - accuracy: 0.7857 - val_loss: 429.1501 - val_accuracy: 0.7143\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 173.2893 - accuracy: 0.5000 - val_loss: 714.0402 - val_accuracy: 0.7143\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 234.8059 - accuracy: 0.5357 - val_loss: 771.8073 - val_accuracy: 0.1429\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 66.9335 - accuracy: 0.8214 - val_loss: 1190.7294 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 208.8467 - accuracy: 0.6786 - val_loss: 1410.3958 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 290.6210 - accuracy: 0.6786 - val_loss: 1306.6881 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 159.5135 - accuracy: 0.6786 - val_loss: 963.5580 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 95.6194 - accuracy: 0.6429 - val_loss: 746.2578 - val_accuracy: 0.2857\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 172.2139 - accuracy: 0.6429 - val_loss: 412.5502 - val_accuracy: 0.7143\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 137.8061 - accuracy: 0.5357 - val_loss: 613.6485 - val_accuracy: 0.2857\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 695ms/step - loss: 5194.4326 - accuracy: 0.3214 - val_loss: 319.3696 - val_accuracy: 1.0000\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1319.2396 - accuracy: 0.3214 - val_loss: 6059.6685 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1896.5682 - accuracy: 0.6429 - val_loss: 8139.7319 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2610.0366 - accuracy: 0.6786 - val_loss: 7605.5659 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2435.0129 - accuracy: 0.6786 - val_loss: 5508.5479 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2001.5898 - accuracy: 0.6786 - val_loss: 3686.7458 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1519.6193 - accuracy: 0.6429 - val_loss: 1598.0385 - val_accuracy: 0.1429\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 923.9878 - accuracy: 0.6429 - val_loss: 333.0505 - val_accuracy: 1.0000\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 871.7592 - accuracy: 0.3214 - val_loss: 779.7575 - val_accuracy: 1.0000\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1359.0944 - accuracy: 0.3214 - val_loss: 900.6251 - val_accuracy: 1.0000\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1374.6033 - accuracy: 0.3214 - val_loss: 535.2082 - val_accuracy: 1.0000\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 877.2811 - accuracy: 0.3214 - val_loss: 238.0329 - val_accuracy: 0.8571\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 476.0143 - accuracy: 0.4643 - val_loss: 1265.5242 - val_accuracy: 0.1429\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 537.6529 - accuracy: 0.6071 - val_loss: 2637.5574 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 773.2098 - accuracy: 0.6786 - val_loss: 3011.6375 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 880.1173 - accuracy: 0.6786 - val_loss: 2311.5852 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 650.4346 - accuracy: 0.6786 - val_loss: 1258.8047 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 577.2568 - accuracy: 0.6429 - val_loss: 331.7227 - val_accuracy: 0.4286\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 377.2677 - accuracy: 0.5000 - val_loss: 691.3854 - val_accuracy: 0.4286\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 491.3998 - accuracy: 0.5357 - val_loss: 932.8204 - val_accuracy: 0.4286\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 683.5937 - accuracy: 0.4643 - val_loss: 671.3044 - val_accuracy: 0.7143\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 543.4124 - accuracy: 0.4286 - val_loss: 20.2583 - val_accuracy: 0.8571\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 284.4895 - accuracy: 0.4643 - val_loss: 509.8977 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 312.8604 - accuracy: 0.5357 - val_loss: 1612.2673 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 379.0517 - accuracy: 0.6786 - val_loss: 1886.2281 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 459.4382 - accuracy: 0.6786 - val_loss: 1252.4757 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 321.0684 - accuracy: 0.7143 - val_loss: 410.3933 - val_accuracy: 0.2857\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 322.9528 - accuracy: 0.5357 - val_loss: 118.2010 - val_accuracy: 0.7143\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 264.7049 - accuracy: 0.4643 - val_loss: 348.9299 - val_accuracy: 0.7143\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 369.6660 - accuracy: 0.5357 - val_loss: 474.6802 - val_accuracy: 0.2857\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 324.3480 - accuracy: 0.5357 - val_loss: 573.7252 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 202.2741 - accuracy: 0.5357 - val_loss: 831.3152 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 197.8057 - accuracy: 0.6429 - val_loss: 971.4980 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 337.1110 - accuracy: 0.6429 - val_loss: 1016.1575 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 191.0681 - accuracy: 0.6786 - val_loss: 798.7462 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 196.9457 - accuracy: 0.6071 - val_loss: 421.4513 - val_accuracy: 0.8571\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 198.9743 - accuracy: 0.5714 - val_loss: 272.3288 - val_accuracy: 0.8571\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 334.2464 - accuracy: 0.3571 - val_loss: 412.7383 - val_accuracy: 0.5714\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 120.1108 - accuracy: 0.5357 - val_loss: 1428.4176 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 325.0169 - accuracy: 0.6786 - val_loss: 1633.5469 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 377.9171 - accuracy: 0.6786 - val_loss: 975.8876 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 211.8465 - accuracy: 0.7143 - val_loss: 335.5276 - val_accuracy: 0.8571\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 250.4173 - accuracy: 0.3929 - val_loss: 329.9120 - val_accuracy: 0.8571\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 242.6647 - accuracy: 0.4643 - val_loss: 837.2714 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 147.1951 - accuracy: 0.6786 - val_loss: 1182.6936 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 199.5243 - accuracy: 0.6786 - val_loss: 855.8404 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 116.0910 - accuracy: 0.7500 - val_loss: 475.6368 - val_accuracy: 0.8571\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 243.8476 - accuracy: 0.5357 - val_loss: 357.2467 - val_accuracy: 0.8571\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 151.7187 - accuracy: 0.5357 - val_loss: 1051.3464 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 246.5887 - accuracy: 0.7500 - val_loss: 1461.4111 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 320.4747 - accuracy: 0.6786 - val_loss: 1469.8010 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 285.6695 - accuracy: 0.6786 - val_loss: 762.5970 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 86.6297 - accuracy: 0.7500 - val_loss: 347.9688 - val_accuracy: 0.8571\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 332.8984 - accuracy: 0.3571 - val_loss: 367.1201 - val_accuracy: 0.8571\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 366.5467 - accuracy: 0.3929 - val_loss: 530.2466 - val_accuracy: 0.8571\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 132.9036 - accuracy: 0.5714 - val_loss: 1364.4535 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 258.9427 - accuracy: 0.6786 - val_loss: 1861.1305 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 457.1602 - accuracy: 0.6786 - val_loss: 2005.9369 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 451.3659 - accuracy: 0.6786 - val_loss: 1610.6029 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 335.0746 - accuracy: 0.6786 - val_loss: 986.5216 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 125.5516 - accuracy: 0.6786 - val_loss: 400.7683 - val_accuracy: 0.8571\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 342.7104 - accuracy: 0.3929 - val_loss: 507.0091 - val_accuracy: 0.8571\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 469.8275 - accuracy: 0.3929 - val_loss: 417.1501 - val_accuracy: 0.8571\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 277.7594 - accuracy: 0.3929 - val_loss: 769.7308 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 131.2859 - accuracy: 0.7143 - val_loss: 1564.4236 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 311.6832 - accuracy: 0.6786 - val_loss: 1642.8802 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 335.9500 - accuracy: 0.6786 - val_loss: 1212.7480 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 317.3934 - accuracy: 0.6786 - val_loss: 581.4240 - val_accuracy: 0.8571\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 110.9160 - accuracy: 0.5714 - val_loss: 853.7925 - val_accuracy: 0.8571\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 399.7164 - accuracy: 0.5714 - val_loss: 1031.2262 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 470.0430 - accuracy: 0.6429 - val_loss: 881.0675 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 176.2955 - accuracy: 0.7500 - val_loss: 1150.6602 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 224.2918 - accuracy: 0.6786 - val_loss: 1432.9963 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 316.8186 - accuracy: 0.6786 - val_loss: 1653.9001 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 333.1202 - accuracy: 0.6786 - val_loss: 1188.0099 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 193.1852 - accuracy: 0.6786 - val_loss: 444.8250 - val_accuracy: 0.8571\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 196.3455 - accuracy: 0.5357 - val_loss: 338.3917 - val_accuracy: 0.8571\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 328.0004 - accuracy: 0.3571 - val_loss: 427.9128 - val_accuracy: 0.7143\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 72.4261 - accuracy: 0.5000 - val_loss: 1187.4877 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 191.1858 - accuracy: 0.6786 - val_loss: 1248.6133 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 205.3468 - accuracy: 0.6786 - val_loss: 601.3635 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 81.5081 - accuracy: 0.6071 - val_loss: 374.9817 - val_accuracy: 0.8571\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 202.4710 - accuracy: 0.3929 - val_loss: 530.1214 - val_accuracy: 0.5714\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 100.0813 - accuracy: 0.5000 - val_loss: 1092.4896 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 149.4092 - accuracy: 0.6786 - val_loss: 1039.9131 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 155.8574 - accuracy: 0.6786 - val_loss: 761.0404 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 152.8707 - accuracy: 0.6429 - val_loss: 469.3706 - val_accuracy: 0.8571\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 80.4866 - accuracy: 0.5714 - val_loss: 710.9594 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 146.4739 - accuracy: 0.6429 - val_loss: 799.0508 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 73.5102 - accuracy: 0.7500 - val_loss: 824.0590 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 180.6456 - accuracy: 0.6429 - val_loss: 602.9294 - val_accuracy: 0.7143\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 74.2455 - accuracy: 0.5357 - val_loss: 791.4780 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 211.4817 - accuracy: 0.6429 - val_loss: 870.8765 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 166.9695 - accuracy: 0.7500 - val_loss: 972.5287 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 119.6933 - accuracy: 0.6786 - val_loss: 902.0312 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 278.6708 - accuracy: 0.6786 - val_loss: 605.3809 - val_accuracy: 0.8571\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 181.3423 - accuracy: 0.5357 - val_loss: 599.9004 - val_accuracy: 0.8571\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 144.6838 - accuracy: 0.5357 - val_loss: 761.0329 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 138.1802 - accuracy: 0.7500 - val_loss: 910.9648 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 116.6755 - accuracy: 0.7143 - val_loss: 833.4753 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 205.8054 - accuracy: 0.6786 - val_loss: 538.4258 - val_accuracy: 0.8571\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 85.1575 - accuracy: 0.5357 - val_loss: 726.6918 - val_accuracy: 0.7143\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 210.1405 - accuracy: 0.5357 - val_loss: 838.7711 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 174.5137 - accuracy: 0.7500 - val_loss: 874.9526 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 92.3292 - accuracy: 0.7500 - val_loss: 840.7183 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 206.1991 - accuracy: 0.6071 - val_loss: 602.3670 - val_accuracy: 0.8571\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 106.7937 - accuracy: 0.5714 - val_loss: 646.7698 - val_accuracy: 0.8571\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 146.0778 - accuracy: 0.6071 - val_loss: 809.4127 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 128.8329 - accuracy: 0.7143 - val_loss: 916.2009 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 107.4073 - accuracy: 0.7143 - val_loss: 819.1966 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 164.6919 - accuracy: 0.6786 - val_loss: 486.7913 - val_accuracy: 0.8571\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 94.9901 - accuracy: 0.5714 - val_loss: 604.0929 - val_accuracy: 0.7143\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 74.1548 - accuracy: 0.6071 - val_loss: 899.8288 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 95.5412 - accuracy: 0.7143 - val_loss: 674.4072 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 44.6919 - accuracy: 0.7500 - val_loss: 476.6091 - val_accuracy: 0.8571\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 123.7000 - accuracy: 0.5000 - val_loss: 654.3691 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 41.2913 - accuracy: 0.7500 - val_loss: 1043.0223 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 133.2138 - accuracy: 0.6786 - val_loss: 874.3204 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 121.3093 - accuracy: 0.7143 - val_loss: 597.4359 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 20.0174 - accuracy: 0.7500 - val_loss: 525.3672 - val_accuracy: 0.8571\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 69.4937 - accuracy: 0.5714 - val_loss: 764.4694 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 46.5754 - accuracy: 0.7143 - val_loss: 620.4416 - val_accuracy: 0.5714\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 46.7685 - accuracy: 0.7500 - val_loss: 740.9431 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 68.1278 - accuracy: 0.7500 - val_loss: 750.3334 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 35.1147 - accuracy: 0.6786 - val_loss: 516.6868 - val_accuracy: 0.8571\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 146.8737 - accuracy: 0.4286 - val_loss: 743.6895 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 105.7062 - accuracy: 0.7857 - val_loss: 1125.6124 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 158.1834 - accuracy: 0.6786 - val_loss: 1187.5685 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 298.2612 - accuracy: 0.6786 - val_loss: 1022.0067 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 184.6275 - accuracy: 0.6786 - val_loss: 828.2084 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 86.6697 - accuracy: 0.6786 - val_loss: 834.5745 - val_accuracy: 0.4286\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 202.4511 - accuracy: 0.7857 - val_loss: 574.7304 - val_accuracy: 0.8571\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 28.2874 - accuracy: 0.6071 - val_loss: 971.8112 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 273.4065 - accuracy: 0.7500 - val_loss: 1120.3925 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 273.4281 - accuracy: 0.7143 - val_loss: 1208.9725 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 180.9944 - accuracy: 0.6786 - val_loss: 896.7518 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 167.9704 - accuracy: 0.7500 - val_loss: 665.6797 - val_accuracy: 0.8571\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 156.4452 - accuracy: 0.5714 - val_loss: 553.2679 - val_accuracy: 0.8571\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 139.1173 - accuracy: 0.5357 - val_loss: 739.8041 - val_accuracy: 0.8571\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 158.7382 - accuracy: 0.6071 - val_loss: 879.3752 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 76.6463 - accuracy: 0.7500 - val_loss: 833.1032 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 130.9040 - accuracy: 0.7500 - val_loss: 636.1484 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 57.1014 - accuracy: 0.6429 - val_loss: 848.1735 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 153.0906 - accuracy: 0.6786 - val_loss: 870.9234 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 164.1814 - accuracy: 0.6786 - val_loss: 615.7987 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 37.9653 - accuracy: 0.6786 - val_loss: 658.5526 - val_accuracy: 0.8571\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 196.2935 - accuracy: 0.5357 - val_loss: 664.0934 - val_accuracy: 0.8571\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 130.6158 - accuracy: 0.6071 - val_loss: 932.9151 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 105.6115 - accuracy: 0.7143 - val_loss: 1066.3728 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 224.2832 - accuracy: 0.7500 - val_loss: 908.4458 - val_accuracy: 0.0000e+00\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 690ms/step - loss: 1418.8115 - accuracy: 0.6786 - val_loss: 65.0894 - val_accuracy: 1.0000\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1227.8436 - accuracy: 0.3214 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1249.0953 - accuracy: 0.3214 - val_loss: 916.9440 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 289.1361 - accuracy: 0.6786 - val_loss: 3127.8738 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 986.0937 - accuracy: 0.6786 - val_loss: 2753.5813 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 957.5157 - accuracy: 0.6786 - val_loss: 1098.2748 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 334.4109 - accuracy: 0.6786 - val_loss: 609.6347 - val_accuracy: 0.8571\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 856.5239 - accuracy: 0.3929 - val_loss: 10.8872 - val_accuracy: 1.0000\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 909.5173 - accuracy: 0.3214 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 720.9183 - accuracy: 0.3214 - val_loss: 164.3295 - val_accuracy: 0.8571\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 102.3027 - accuracy: 0.5714 - val_loss: 2005.7531 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 642.9878 - accuracy: 0.6786 - val_loss: 2689.7864 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 871.1600 - accuracy: 0.6786 - val_loss: 2116.1841 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 663.4466 - accuracy: 0.6786 - val_loss: 1045.2230 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 442.1487 - accuracy: 0.6786 - val_loss: 41.6909 - val_accuracy: 0.8571\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 304.5692 - accuracy: 0.5357 - val_loss: 78.6517 - val_accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 531.6797 - accuracy: 0.3929 - val_loss: 457.0311 - val_accuracy: 0.8571\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 636.0396 - accuracy: 0.5357 - val_loss: 102.7479 - val_accuracy: 0.8571\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 207.6069 - accuracy: 0.5357 - val_loss: 720.9334 - val_accuracy: 0.2857\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 349.8910 - accuracy: 0.6071 - val_loss: 1159.7433 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 380.4771 - accuracy: 0.6786 - val_loss: 1531.2064 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 453.1872 - accuracy: 0.6786 - val_loss: 1225.1316 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 411.9174 - accuracy: 0.6786 - val_loss: 916.1938 - val_accuracy: 0.2857\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 440.1291 - accuracy: 0.5714 - val_loss: 130.2811 - val_accuracy: 0.8571\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 182.4477 - accuracy: 0.5357 - val_loss: 134.8205 - val_accuracy: 0.8571\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 229.6431 - accuracy: 0.5357 - val_loss: 410.8710 - val_accuracy: 0.5714\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 160.0459 - accuracy: 0.5714 - val_loss: 1105.8888 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 292.4767 - accuracy: 0.6786 - val_loss: 1104.9747 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 329.1601 - accuracy: 0.6786 - val_loss: 830.6138 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 275.9994 - accuracy: 0.7500 - val_loss: 242.0299 - val_accuracy: 0.8571\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 73.7569 - accuracy: 0.5714 - val_loss: 380.8115 - val_accuracy: 0.8571\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 224.4311 - accuracy: 0.5714 - val_loss: 241.6413 - val_accuracy: 0.8571\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 120.8974 - accuracy: 0.5357 - val_loss: 604.3659 - val_accuracy: 0.4286\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 185.9927 - accuracy: 0.6429 - val_loss: 722.0585 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 135.7736 - accuracy: 0.6786 - val_loss: 447.5561 - val_accuracy: 0.4286\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 98.7514 - accuracy: 0.6429 - val_loss: 269.7785 - val_accuracy: 0.8571\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 167.5801 - accuracy: 0.5357 - val_loss: 268.4457 - val_accuracy: 0.8571\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 168.9114 - accuracy: 0.5357 - val_loss: 382.9102 - val_accuracy: 0.7143\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 82.2625 - accuracy: 0.6071 - val_loss: 879.0825 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 211.6221 - accuracy: 0.6786 - val_loss: 1021.2079 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 220.1926 - accuracy: 0.6786 - val_loss: 675.7968 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 93.0016 - accuracy: 0.6786 - val_loss: 299.4220 - val_accuracy: 0.8571\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 170.0458 - accuracy: 0.5357 - val_loss: 284.4750 - val_accuracy: 0.8571\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 210.2937 - accuracy: 0.5357 - val_loss: 540.6366 - val_accuracy: 0.7143\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 182.6737 - accuracy: 0.5714 - val_loss: 689.9988 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 113.3142 - accuracy: 0.6786 - val_loss: 890.2690 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 163.7530 - accuracy: 0.6786 - val_loss: 431.6121 - val_accuracy: 0.7143\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 41.1951 - accuracy: 0.6071 - val_loss: 623.6808 - val_accuracy: 0.7143\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 127.1733 - accuracy: 0.6429 - val_loss: 557.5438 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 62.7966 - accuracy: 0.7500 - val_loss: 533.9238 - val_accuracy: 0.5714\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 67.0124 - accuracy: 0.6786 - val_loss: 465.3225 - val_accuracy: 0.7143\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 104.5321 - accuracy: 0.5714 - val_loss: 405.5549 - val_accuracy: 0.7143\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 97.4150 - accuracy: 0.5714 - val_loss: 352.4160 - val_accuracy: 0.7143\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 91.1956 - accuracy: 0.5357 - val_loss: 634.7199 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 80.1402 - accuracy: 0.6786 - val_loss: 644.6660 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 75.0553 - accuracy: 0.6786 - val_loss: 325.6727 - val_accuracy: 0.8571\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 153.5563 - accuracy: 0.5357 - val_loss: 325.1234 - val_accuracy: 0.8571\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 186.0644 - accuracy: 0.5357 - val_loss: 526.3741 - val_accuracy: 0.7143\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 124.0259 - accuracy: 0.6071 - val_loss: 836.1424 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 169.4196 - accuracy: 0.6786 - val_loss: 953.2165 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 177.9296 - accuracy: 0.6786 - val_loss: 828.7650 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 116.5929 - accuracy: 0.6786 - val_loss: 630.7571 - val_accuracy: 0.7143\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 105.4778 - accuracy: 0.6786 - val_loss: 470.0616 - val_accuracy: 0.7143\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 184.0591 - accuracy: 0.5714 - val_loss: 603.3078 - val_accuracy: 0.7143\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 241.8004 - accuracy: 0.5714 - val_loss: 513.5627 - val_accuracy: 0.7143\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 66.3751 - accuracy: 0.6071 - val_loss: 1178.2926 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 291.5945 - accuracy: 0.6786 - val_loss: 1428.7623 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 307.9503 - accuracy: 0.6786 - val_loss: 999.8754 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 172.2410 - accuracy: 0.6786 - val_loss: 396.4851 - val_accuracy: 0.7143\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 125.7180 - accuracy: 0.5357 - val_loss: 352.4456 - val_accuracy: 0.8571\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 234.1357 - accuracy: 0.5357 - val_loss: 524.2681 - val_accuracy: 0.7143\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 153.1932 - accuracy: 0.5714 - val_loss: 708.5847 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 129.6257 - accuracy: 0.7500 - val_loss: 851.5319 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 146.9073 - accuracy: 0.6786 - val_loss: 615.5869 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 74.2430 - accuracy: 0.6786 - val_loss: 353.5443 - val_accuracy: 0.7143\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 140.3502 - accuracy: 0.5357 - val_loss: 367.0356 - val_accuracy: 0.7143\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 145.8109 - accuracy: 0.5357 - val_loss: 587.2059 - val_accuracy: 0.4286\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 76.6516 - accuracy: 0.6786 - val_loss: 960.9802 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 191.6499 - accuracy: 0.6786 - val_loss: 1021.4681 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 187.1348 - accuracy: 0.6786 - val_loss: 574.3337 - val_accuracy: 0.1429\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 38.2938 - accuracy: 0.6786 - val_loss: 345.1060 - val_accuracy: 0.8571\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 212.9314 - accuracy: 0.5357 - val_loss: 357.6619 - val_accuracy: 0.8571\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 236.9435 - accuracy: 0.5357 - val_loss: 359.6355 - val_accuracy: 0.7143\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 97.4889 - accuracy: 0.5357 - val_loss: 929.3431 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 172.2516 - accuracy: 0.6786 - val_loss: 1113.0272 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 255.0023 - accuracy: 0.6786 - val_loss: 1003.4967 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 200.5016 - accuracy: 0.6786 - val_loss: 370.3818 - val_accuracy: 0.7143\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 73.0451 - accuracy: 0.5714 - val_loss: 335.7530 - val_accuracy: 0.8571\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 159.0146 - accuracy: 0.5357 - val_loss: 538.2999 - val_accuracy: 0.7143\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 203.0770 - accuracy: 0.5714 - val_loss: 371.0985 - val_accuracy: 0.7143\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 75.7677 - accuracy: 0.6071 - val_loss: 889.4499 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 205.8574 - accuracy: 0.6786 - val_loss: 1248.2673 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 261.2378 - accuracy: 0.6786 - val_loss: 840.1725 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 130.8045 - accuracy: 0.6786 - val_loss: 367.6643 - val_accuracy: 0.7143\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 150.7637 - accuracy: 0.5357 - val_loss: 355.7100 - val_accuracy: 0.7143\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 197.1154 - accuracy: 0.5357 - val_loss: 529.4474 - val_accuracy: 0.7143\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 25.8205 - accuracy: 0.6429 - val_loss: 1533.3905 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 330.3318 - accuracy: 0.6786 - val_loss: 1634.3240 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 362.3521 - accuracy: 0.6786 - val_loss: 895.9963 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 129.9760 - accuracy: 0.6786 - val_loss: 416.0268 - val_accuracy: 0.7143\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 106.8436 - accuracy: 0.5714 - val_loss: 546.5139 - val_accuracy: 0.7143\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 291.9431 - accuracy: 0.5357 - val_loss: 319.2137 - val_accuracy: 0.8571\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 181.5008 - accuracy: 0.5357 - val_loss: 466.4360 - val_accuracy: 0.7143\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 101.0878 - accuracy: 0.5714 - val_loss: 678.4383 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 78.7096 - accuracy: 0.6786 - val_loss: 493.6747 - val_accuracy: 0.7143\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 74.3889 - accuracy: 0.6429 - val_loss: 307.3757 - val_accuracy: 0.8571\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 153.5918 - accuracy: 0.5357 - val_loss: 442.8196 - val_accuracy: 0.7143\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 213.4405 - accuracy: 0.5357 - val_loss: 346.4927 - val_accuracy: 0.7143\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 66.9865 - accuracy: 0.6071 - val_loss: 1006.6957 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 276.2347 - accuracy: 0.6786 - val_loss: 1267.0059 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 285.5682 - accuracy: 0.6786 - val_loss: 848.8698 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 144.6829 - accuracy: 0.6786 - val_loss: 256.1976 - val_accuracy: 0.8571\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 183.8811 - accuracy: 0.5357 - val_loss: 200.8617 - val_accuracy: 0.8571\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 310.3390 - accuracy: 0.5357 - val_loss: 279.0998 - val_accuracy: 0.8571\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 177.4042 - accuracy: 0.5357 - val_loss: 703.4733 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 144.6396 - accuracy: 0.6786 - val_loss: 1241.7598 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 303.9661 - accuracy: 0.6786 - val_loss: 1196.6200 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 364.3725 - accuracy: 0.6786 - val_loss: 798.1678 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 230.1683 - accuracy: 0.6786 - val_loss: 451.2263 - val_accuracy: 0.7143\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 163.8090 - accuracy: 0.5714 - val_loss: 291.7656 - val_accuracy: 0.8571\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 218.2427 - accuracy: 0.5357 - val_loss: 192.7805 - val_accuracy: 0.8571\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 278.8397 - accuracy: 0.5357 - val_loss: 255.0602 - val_accuracy: 0.8571\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 133.7815 - accuracy: 0.5357 - val_loss: 898.7198 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 178.2009 - accuracy: 0.6786 - val_loss: 1543.4911 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 382.4832 - accuracy: 0.6786 - val_loss: 1336.5370 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 313.3645 - accuracy: 0.6786 - val_loss: 813.0690 - val_accuracy: 0.4286\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 315.5369 - accuracy: 0.6429 - val_loss: 225.9848 - val_accuracy: 0.8571\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 236.1186 - accuracy: 0.5357 - val_loss: 193.7408 - val_accuracy: 0.8571\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 284.7428 - accuracy: 0.5357 - val_loss: 263.9622 - val_accuracy: 0.8571\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 165.0285 - accuracy: 0.5357 - val_loss: 702.2195 - val_accuracy: 0.1429\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 166.8757 - accuracy: 0.6786 - val_loss: 1013.3032 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 221.9508 - accuracy: 0.6786 - val_loss: 967.9372 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 309.9292 - accuracy: 0.6786 - val_loss: 649.1907 - val_accuracy: 0.7143\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 205.8043 - accuracy: 0.6429 - val_loss: 381.4643 - val_accuracy: 0.7143\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 142.8614 - accuracy: 0.6071 - val_loss: 472.4501 - val_accuracy: 0.7143\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 280.1910 - accuracy: 0.5357 - val_loss: 283.1593 - val_accuracy: 0.8571\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 184.2336 - accuracy: 0.5357 - val_loss: 420.9880 - val_accuracy: 0.7143\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 99.7505 - accuracy: 0.6071 - val_loss: 789.7871 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 125.4028 - accuracy: 0.6786 - val_loss: 770.4088 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 112.2673 - accuracy: 0.6786 - val_loss: 373.9417 - val_accuracy: 0.7143\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 111.0032 - accuracy: 0.5714 - val_loss: 340.0870 - val_accuracy: 0.7143\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 167.5365 - accuracy: 0.5357 - val_loss: 443.5386 - val_accuracy: 0.7143\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 112.5318 - accuracy: 0.6071 - val_loss: 622.0881 - val_accuracy: 0.4286\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 75.5066 - accuracy: 0.6429 - val_loss: 736.0846 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 82.9205 - accuracy: 0.6786 - val_loss: 468.5378 - val_accuracy: 0.7143\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 57.0184 - accuracy: 0.6071 - val_loss: 378.9869 - val_accuracy: 0.7143\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 120.0526 - accuracy: 0.5714 - val_loss: 509.1807 - val_accuracy: 0.7143\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 114.3775 - accuracy: 0.5714 - val_loss: 568.7897 - val_accuracy: 0.1429\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 36.4321 - accuracy: 0.6786 - val_loss: 583.3350 - val_accuracy: 0.5714\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 69.4978 - accuracy: 0.6786 - val_loss: 457.2044 - val_accuracy: 0.7143\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 657ms/step - loss: 7847.1108 - accuracy: 0.5357 - val_loss: 5641.1465 - val_accuracy: 1.0000\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5465.1099 - accuracy: 0.5357 - val_loss: 2284.5740 - val_accuracy: 1.0000\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3104.4866 - accuracy: 0.5357 - val_loss: 17.9962 - val_accuracy: 1.0000\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1309.2899 - accuracy: 0.5357 - val_loss: 784.3503 - val_accuracy: 1.0000\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1053.6453 - accuracy: 0.5714 - val_loss: 2439.5186 - val_accuracy: 0.7500\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1779.2517 - accuracy: 0.5000 - val_loss: 3268.7495 - val_accuracy: 0.2500\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1883.6013 - accuracy: 0.5714 - val_loss: 3410.7830 - val_accuracy: 0.1250\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1678.7599 - accuracy: 0.5000 - val_loss: 3021.2544 - val_accuracy: 0.1250\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1451.1635 - accuracy: 0.5000 - val_loss: 2046.3772 - val_accuracy: 0.1250\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1082.4706 - accuracy: 0.5714 - val_loss: 627.0543 - val_accuracy: 0.3750\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 558.3478 - accuracy: 0.5000 - val_loss: 59.9633 - val_accuracy: 1.0000\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 558.9273 - accuracy: 0.5357 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 911.8149 - accuracy: 0.5357 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1012.5903 - accuracy: 0.4643 - val_loss: 15.9888 - val_accuracy: 1.0000\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 862.6183 - accuracy: 0.5357 - val_loss: 94.6225 - val_accuracy: 1.0000\n",
      "Epoch 16/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 615.8391 - accuracy: 0.5357 - val_loss: 220.1101 - val_accuracy: 0.7500\n",
      "Epoch 17/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 371.4114 - accuracy: 0.5000 - val_loss: 652.7906 - val_accuracy: 0.2500\n",
      "Epoch 18/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 459.7033 - accuracy: 0.5357 - val_loss: 1081.0980 - val_accuracy: 0.1250\n",
      "Epoch 19/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 579.8245 - accuracy: 0.6786 - val_loss: 1094.0669 - val_accuracy: 0.1250\n",
      "Epoch 20/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 567.3002 - accuracy: 0.6429 - val_loss: 865.2921 - val_accuracy: 0.3750\n",
      "Epoch 21/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 502.3071 - accuracy: 0.5714 - val_loss: 387.6080 - val_accuracy: 0.6250\n",
      "Epoch 22/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 333.3301 - accuracy: 0.5714 - val_loss: 67.8103 - val_accuracy: 0.7500\n",
      "Epoch 23/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 180.5389 - accuracy: 0.5714 - val_loss: 290.9261 - val_accuracy: 1.0000\n",
      "Epoch 24/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 375.5352 - accuracy: 0.5714 - val_loss: 40.6920 - val_accuracy: 1.0000\n",
      "Epoch 25/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 377.8680 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 26/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 476.8085 - accuracy: 0.4643 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 27/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 326.5848 - accuracy: 0.5714 - val_loss: 93.9256 - val_accuracy: 0.7500\n",
      "Epoch 28/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 124.8492 - accuracy: 0.5714 - val_loss: 753.8662 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 337.6775 - accuracy: 0.6071 - val_loss: 890.2288 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 366.9070 - accuracy: 0.6071 - val_loss: 509.2310 - val_accuracy: 0.1250\n",
      "Epoch 31/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 247.9442 - accuracy: 0.6786 - val_loss: 260.0938 - val_accuracy: 0.7500\n",
      "Epoch 32/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 220.6335 - accuracy: 0.5714 - val_loss: 57.3042 - val_accuracy: 0.8750\n",
      "Epoch 33/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 103.6177 - accuracy: 0.5714 - val_loss: 237.8714 - val_accuracy: 1.0000\n",
      "Epoch 34/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 245.3826 - accuracy: 0.6071 - val_loss: 54.3797 - val_accuracy: 1.0000\n",
      "Epoch 35/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 222.4842 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 36/150\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 268.6653 - accuracy: 0.5357 - val_loss: 7.9280e-36 - val_accuracy: 1.0000\n",
      "Epoch 37/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 168.5809 - accuracy: 0.6071 - val_loss: 197.3772 - val_accuracy: 0.1250\n",
      "Epoch 38/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 126.1198 - accuracy: 0.6071 - val_loss: 523.2481 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 222.8615 - accuracy: 0.6071 - val_loss: 346.8611 - val_accuracy: 0.1250\n",
      "Epoch 40/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 162.4892 - accuracy: 0.6071 - val_loss: 34.9782 - val_accuracy: 0.7500\n",
      "Epoch 41/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 82.7989 - accuracy: 0.5357 - val_loss: 62.9989 - val_accuracy: 1.0000\n",
      "Epoch 42/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 162.0338 - accuracy: 0.6071 - val_loss: 3.5452 - val_accuracy: 1.0000\n",
      "Epoch 43/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 193.3230 - accuracy: 0.6071 - val_loss: 3.6310 - val_accuracy: 1.0000\n",
      "Epoch 44/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 131.3327 - accuracy: 0.6071 - val_loss: 147.3943 - val_accuracy: 0.2500\n",
      "Epoch 45/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 82.3907 - accuracy: 0.6071 - val_loss: 277.6418 - val_accuracy: 0.1250\n",
      "Epoch 46/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 124.6593 - accuracy: 0.6071 - val_loss: 213.2933 - val_accuracy: 0.1250\n",
      "Epoch 47/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 76.6444 - accuracy: 0.6429 - val_loss: 40.4241 - val_accuracy: 0.7500\n",
      "Epoch 48/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 41.3732 - accuracy: 0.5714 - val_loss: 13.5426 - val_accuracy: 0.8750\n",
      "Epoch 49/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 107.6337 - accuracy: 0.6071 - val_loss: 39.7406 - val_accuracy: 0.8750\n",
      "Epoch 50/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 63.0704 - accuracy: 0.6071 - val_loss: 290.5977 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 119.9084 - accuracy: 0.6071 - val_loss: 167.4802 - val_accuracy: 0.1250\n",
      "Epoch 52/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 81.5176 - accuracy: 0.6071 - val_loss: 111.5976 - val_accuracy: 0.8750\n",
      "Epoch 53/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 124.7591 - accuracy: 0.6071 - val_loss: 20.6244 - val_accuracy: 1.0000\n",
      "Epoch 54/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 142.3673 - accuracy: 0.6071 - val_loss: 57.0974 - val_accuracy: 0.8750\n",
      "Epoch 55/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 68.5655 - accuracy: 0.6071 - val_loss: 316.4732 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 111.7829 - accuracy: 0.6071 - val_loss: 40.8002 - val_accuracy: 0.7500\n",
      "Epoch 57/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 22.0457 - accuracy: 0.5714 - val_loss: 37.0542 - val_accuracy: 0.8750\n",
      "Epoch 58/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 91.3754 - accuracy: 0.6071 - val_loss: 167.1777 - val_accuracy: 0.8750\n",
      "Epoch 59/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 104.5861 - accuracy: 0.6071 - val_loss: 171.2367 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 63.1637 - accuracy: 0.6071 - val_loss: 109.3421 - val_accuracy: 0.1250\n",
      "Epoch 61/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 35.8551 - accuracy: 0.6429 - val_loss: 37.5940 - val_accuracy: 0.8750\n",
      "Epoch 62/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 122.6310 - accuracy: 0.6071 - val_loss: 48.8311 - val_accuracy: 0.8750\n",
      "Epoch 63/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 109.9791 - accuracy: 0.6071 - val_loss: 48.7897 - val_accuracy: 0.8750\n",
      "Epoch 64/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 25.9727 - accuracy: 0.6429 - val_loss: 610.2693 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 211.9821 - accuracy: 0.6071 - val_loss: 504.5645 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 174.6356 - accuracy: 0.6071 - val_loss: 100.9398 - val_accuracy: 0.8750\n",
      "Epoch 67/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 70.9598 - accuracy: 0.6429 - val_loss: 49.8408 - val_accuracy: 0.8750\n",
      "Epoch 68/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 148.3572 - accuracy: 0.6071 - val_loss: 299.1582 - val_accuracy: 0.8750\n",
      "Epoch 69/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 258.8114 - accuracy: 0.6071 - val_loss: 195.8863 - val_accuracy: 0.8750\n",
      "Epoch 70/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 114.8057 - accuracy: 0.6071 - val_loss: 370.0446 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 203.1562 - accuracy: 0.6071 - val_loss: 503.5371 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 292.0149 - accuracy: 0.6071 - val_loss: 258.3505 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 129.7836 - accuracy: 0.7143 - val_loss: 366.0611 - val_accuracy: 0.7500\n",
      "Epoch 74/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 206.6922 - accuracy: 0.5714 - val_loss: 452.8541 - val_accuracy: 0.8750\n",
      "Epoch 75/150\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 319.7509 - accuracy: 0.6071 - val_loss: 240.2129 - val_accuracy: 0.8750\n",
      "Epoch 76/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 189.1732 - accuracy: 0.6429 - val_loss: 23.8776 - val_accuracy: 0.8750\n",
      "Epoch 77/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 106.0442 - accuracy: 0.6071 - val_loss: 258.8166 - val_accuracy: 0.8750\n",
      "Epoch 78/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 269.3799 - accuracy: 0.6071 - val_loss: 188.9249 - val_accuracy: 0.8750\n",
      "Epoch 79/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 197.3393 - accuracy: 0.6071 - val_loss: 49.1757 - val_accuracy: 0.8750\n",
      "Epoch 80/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 70.9048 - accuracy: 0.6071 - val_loss: 258.0944 - val_accuracy: 0.8750\n",
      "Epoch 81/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 179.7113 - accuracy: 0.6071 - val_loss: 159.6342 - val_accuracy: 0.7500\n",
      "Epoch 82/150\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 110.1558 - accuracy: 0.5714 - val_loss: 76.4165 - val_accuracy: 0.7500\n",
      "Epoch 83/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 104.8341 - accuracy: 0.5714 - val_loss: 211.2658 - val_accuracy: 0.7500\n",
      "Epoch 84/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 178.5831 - accuracy: 0.6071 - val_loss: 12.6364 - val_accuracy: 1.0000\n",
      "Epoch 85/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 86.7508 - accuracy: 0.6071 - val_loss: 88.9133 - val_accuracy: 1.0000\n",
      "Epoch 86/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 114.8864 - accuracy: 0.6071 - val_loss: 181.4266 - val_accuracy: 1.0000\n",
      "Epoch 87/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 148.1141 - accuracy: 0.6071 - val_loss: 4.8525 - val_accuracy: 1.0000\n",
      "Epoch 88/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 36.4134 - accuracy: 0.6071 - val_loss: 294.2827 - val_accuracy: 0.6250\n",
      "Epoch 89/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 199.6688 - accuracy: 0.5714 - val_loss: 281.0043 - val_accuracy: 0.7500\n",
      "Epoch 90/150\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 207.5696 - accuracy: 0.5714 - val_loss: 4.4237e-15 - val_accuracy: 1.0000\n",
      "Epoch 91/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 46.6254 - accuracy: 0.6071 - val_loss: 350.8438 - val_accuracy: 0.1250\n",
      "Epoch 92/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 187.9158 - accuracy: 0.6429 - val_loss: 478.4574 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 203.1938 - accuracy: 0.6071 - val_loss: 511.6408 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 190.9393 - accuracy: 0.6071 - val_loss: 88.0707 - val_accuracy: 0.7500\n",
      "Epoch 95/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 71.3206 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 96/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 153.8723 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 97/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 130.8973 - accuracy: 0.6071 - val_loss: 79.9428 - val_accuracy: 0.2500\n",
      "Epoch 98/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 44.1200 - accuracy: 0.6071 - val_loss: 227.1226 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 82.4301 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 100/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 76.1371 - accuracy: 0.6071 - val_loss: 2.0186e-28 - val_accuracy: 1.0000\n",
      "Epoch 101/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 70.8205 - accuracy: 0.6071 - val_loss: 200.8125 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 72.1985 - accuracy: 0.5714 - val_loss: 97.8075 - val_accuracy: 0.1250\n",
      "Epoch 103/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 40.6179 - accuracy: 0.6429 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 104/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 111.3006 - accuracy: 0.6071 - val_loss: 3.2710e-14 - val_accuracy: 1.0000\n",
      "Epoch 105/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 103.7986 - accuracy: 0.6071 - val_loss: 182.2245 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 76.9042 - accuracy: 0.6786 - val_loss: 382.7673 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 132.8204 - accuracy: 0.6071 - val_loss: 294.4025 - val_accuracy: 1.0000\n",
      "Epoch 108/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 199.1762 - accuracy: 0.6429 - val_loss: 41.0658 - val_accuracy: 1.0000\n",
      "Epoch 109/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 158.0108 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 110/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 165.6145 - accuracy: 0.6071 - val_loss: 270.5619 - val_accuracy: 1.0000\n",
      "Epoch 111/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 221.4821 - accuracy: 0.6071 - val_loss: 356.3284 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 180.6982 - accuracy: 0.6786 - val_loss: 246.6471 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 88.3036 - accuracy: 0.6071 - val_loss: 249.4968 - val_accuracy: 1.0000\n",
      "Epoch 114/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 217.3545 - accuracy: 0.6071 - val_loss: 56.6477 - val_accuracy: 1.0000\n",
      "Epoch 115/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 223.9872 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 116/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 202.3605 - accuracy: 0.6071 - val_loss: 0.6742 - val_accuracy: 1.0000\n",
      "Epoch 117/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 126.7958 - accuracy: 0.6071 - val_loss: 461.3711 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 251.2232 - accuracy: 0.6786 - val_loss: 609.1448 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 251.7581 - accuracy: 0.6071 - val_loss: 569.4326 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 228.8184 - accuracy: 0.6071 - val_loss: 107.2629 - val_accuracy: 0.8750\n",
      "Epoch 121/150\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 86.5228 - accuracy: 0.5714 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 122/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 167.8590 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 123/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 198.4621 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 124/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 103.1901 - accuracy: 0.6071 - val_loss: 362.3917 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 134.0441 - accuracy: 0.6071 - val_loss: 450.7675 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 164.2496 - accuracy: 0.6071 - val_loss: 87.6850 - val_accuracy: 1.0000\n",
      "Epoch 127/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 72.7783 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 128/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 190.4035 - accuracy: 0.6071 - val_loss: 7.6852e-24 - val_accuracy: 1.0000\n",
      "Epoch 129/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 237.2235 - accuracy: 0.6071 - val_loss: 203.2686 - val_accuracy: 1.0000\n",
      "Epoch 130/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 230.1131 - accuracy: 0.6071 - val_loss: 181.8457 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 82.3952 - accuracy: 0.7143 - val_loss: 578.8397 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 233.6758 - accuracy: 0.6071 - val_loss: 796.6179 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 339.1118 - accuracy: 0.6071 - val_loss: 646.1116 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 257.8576 - accuracy: 0.6071 - val_loss: 321.2232 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 117.3502 - accuracy: 0.6071 - val_loss: 104.6822 - val_accuracy: 1.0000\n",
      "Epoch 136/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 197.7068 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 137/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 354.3258 - accuracy: 0.4643 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 138/150\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 368.6713 - accuracy: 0.4643 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 139/150\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 201.9622 - accuracy: 0.6071 - val_loss: 125.9173 - val_accuracy: 0.3750\n",
      "Epoch 140/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 91.1810 - accuracy: 0.5714 - val_loss: 568.6479 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 235.6570 - accuracy: 0.6071 - val_loss: 646.2330 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 294.5722 - accuracy: 0.6071 - val_loss: 481.6592 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 252.2887 - accuracy: 0.5714 - val_loss: 28.4429 - val_accuracy: 0.8750\n",
      "Epoch 144/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 71.2925 - accuracy: 0.5714 - val_loss: 140.6867 - val_accuracy: 1.0000\n",
      "Epoch 145/150\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 189.3481 - accuracy: 0.6071 - val_loss: 110.5644 - val_accuracy: 1.0000\n",
      "Epoch 146/150\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 248.4826 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 147/150\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 230.9944 - accuracy: 0.6071 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 148/150\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 159.1303 - accuracy: 0.6071 - val_loss: 140.2366 - val_accuracy: 0.2500\n",
      "Epoch 149/150\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 79.9499 - accuracy: 0.6071 - val_loss: 621.5657 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 228.1367 - accuracy: 0.6071 - val_loss: 540.0457 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Train Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "history = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "\n",
    "x_train_best = []\n",
    "y_train_best = []\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x_train = np.argmax(x_train, axis=1) \n",
    "\n",
    "for i, (train, val) in enumerate(cv.split(y_train, x_train)):\n",
    "\n",
    "    X_train, X_val = y[train], y[val]\n",
    "    y_train, y_val = x[train], x[val]\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_train)\n",
    "    y_train = le.transform(y_train)\n",
    "    y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "    \n",
    "    classifier = classifier_modeling()\n",
    "\n",
    "    # Fit model\n",
    "    classifier.fit(X_train, y_train, validation_split=0.2, epochs=150, batch_size=40)\n",
    "\n",
    "    # Save model\n",
    "    #with open('model\\\\ann_wofill', 'wb') as f:\n",
    "    #    pickle.dump(classifier, f)\n",
    "\n",
    "    # Predict\n",
    "    predict = classifier.predict(X_val)\n",
    "    predict = np.argmax(predict, axis=1)\n",
    "\n",
    "    viz = RocCurveDisplay.from_predictions(y_val, predict, name='ROC fold {}'.format(i),\n",
    "                           alpha=0.3, lw=1, ax=ax)                     \n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "    x_train_best.append(x_train)\n",
    "    y_train_best.append(y_train)\n",
    "\n",
    "\n",
    "# mean line\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"r\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "\n",
    "# std\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"yellow\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.savefig(\n",
    "   'graph\\\\ann_wofill_graph.jpg')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#findind best x_train y_train\n",
    "\n",
    "index_max_auc = aucs.index(max(aucs))\n",
    "index_max_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_test)\n",
    "# print(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 11) for input KerasTensor(type_spec=TensorSpec(shape=(None, 11), dtype=tf.float32, name='dense_532_input'), name='dense_532_input', description=\"created by layer 'dense_532_input'\"), but it was called on an input with incompatible shape (None,).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_108\" (type Sequential).\n    \n    Input 0 of layer \"dense_532\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=int64)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\CN thammasart\\CN22021\\CN240\\CN240-ML\\Deep_Learning\\ANN_real_filled_1.ipynb Cell 12'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_real_filled_1.ipynb#ch0000010?line=4'>5</a>\u001b[0m classifier \u001b[39m=\u001b[39m classifier_modeling()\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_real_filled_1.ipynb#ch0000010?line=6'>7</a>\u001b[0m \u001b[39m# Fit model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_real_filled_1.ipynb#ch0000010?line=7'>8</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(x_train_best[index_max_auc], y_train_best[index_max_auc], validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_real_filled_1.ipynb#ch0000010?line=8'>9</a>\u001b[0m                epochs\u001b[39m=\u001b[39;49m\u001b[39m150\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_real_filled_1.ipynb#ch0000010?line=9'>10</a>\u001b[0m \u001b[39m# Report\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_real_filled_1.ipynb#ch0000010?line=10'>11</a>\u001b[0m predict \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(y_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\input_spec.py\", line 228, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_108\" (type Sequential).\n    \n    Input 0 of layer \"dense_532\" is incompatible with the layer: expected min_ndim=2, found ndim=1. Full shape received: (None,)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=int64)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "classifier = classifier_modeling()\n",
    "\n",
    "# Fit model\n",
    "classifier.fit(x_train_best[index_max_auc], y_train_best[index_max_auc], validation_split=0.2,\n",
    "               epochs=150, batch_size=40)\n",
    "# Report\n",
    "predict = classifier.predict(y_test)\n",
    "predict = np.argmax(predict, axis=1)\n",
    "# x_test = x_test[:,0]\n",
    "\n",
    "print(predict)\n",
    "print(x_test)\n",
    "\n",
    "print(classification_report(x_test, predict))\n",
    "\n",
    "conf = confusion_matrix(x_test, predict)\n",
    "sensitivity = conf[1, 1]/(conf[0, 1] + conf[1, 1])\n",
    "specificity = conf[0, 0]/(conf[0, 0] + conf[1, 0])\n",
    "acc = accuracy_score(predict, x_test)\n",
    "\n",
    "print('Sensitivity : ', sensitivity)\n",
    "print('Specificity : ', specificity)\n",
    "print('Accuracy : ', acc)\n",
    "print('Precision : %.3f' % precision_score(predict, x_test))\n",
    "print('F1 Score : %.3f' % f1_score(predict, x_test))\n",
    "print('                         non-depressed        depressed   ')\n",
    "print('Actual non=depressed     %6d' %\n",
    "      conf[0, 0] + \"            %5d\" % conf[0, 1])\n",
    "print('Actual depressed         %6d' %\n",
    "      conf[1, 0] + \"            %5d\" % conf[1, 1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf.shape[0]):\n",
    "    for j in range(conf.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf[i, j], va='center',\n",
    "                ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix NN(relu, selu)', fontsize=18) #change by your output's activation function\n",
    "plt.savefig(\n",
    "   'matrix\\\\ann_filled_matrix.jpg')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae305f4dee197bdc0c916b004f5792eca0da2328f3eb5fe8e31b308819fc7eb2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
