{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\CN thammasart\\CN22021\\CN240\\CN240-ML\\Deep_Learning\\ANN_filled.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000000?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000000?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000000?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m to_categorical\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000000?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dropout\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000000?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'to_categorical' from 'keras.utils' (C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "\n",
    "import math\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "#cross-validation and plot ROC curves\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#NN\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import f1_score ,accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>afftype</th>\n",
       "      <th>melanch</th>\n",
       "      <th>inpatient</th>\n",
       "      <th>edu</th>\n",
       "      <th>marriage</th>\n",
       "      <th>work</th>\n",
       "      <th>madrs1</th>\n",
       "      <th>5days_sleep_time_activity</th>\n",
       "      <th>5days_day_time_activity</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>10693.6</td>\n",
       "      <td>228824.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>48771.2</td>\n",
       "      <td>239278.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>43211.0</td>\n",
       "      <td>317726.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>56892.4</td>\n",
       "      <td>194298.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>31303.8</td>\n",
       "      <td>200302.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>26634.2</td>\n",
       "      <td>240767.8</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>61643.6</td>\n",
       "      <td>335598.2</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>34374.2</td>\n",
       "      <td>284320.6</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>42992.0</td>\n",
       "      <td>203120.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>25811.0</td>\n",
       "      <td>482765.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>50147.0</td>\n",
       "      <td>153494.6</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>34576.6</td>\n",
       "      <td>228420.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>62330.8</td>\n",
       "      <td>291661.6</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>42878.0</td>\n",
       "      <td>72237.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>42016.2</td>\n",
       "      <td>164387.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>17938.8</td>\n",
       "      <td>397321.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>24344.6</td>\n",
       "      <td>85835.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>40038.4</td>\n",
       "      <td>64142.0</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>17014.0</td>\n",
       "      <td>189703.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>10301.8</td>\n",
       "      <td>84496.8</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>4627.0</td>\n",
       "      <td>91568.8</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>18023.6</td>\n",
       "      <td>183974.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>29581.2</td>\n",
       "      <td>235831.4</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>62703.4</td>\n",
       "      <td>257546.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>51929.0</td>\n",
       "      <td>475113.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>34742.8</td>\n",
       "      <td>347817.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>23481.0</td>\n",
       "      <td>255676.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>49904.8</td>\n",
       "      <td>398723.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>103205.0</td>\n",
       "      <td>431101.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>115329.4</td>\n",
       "      <td>479807.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>124525.0</td>\n",
       "      <td>467588.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>19149.8</td>\n",
       "      <td>169119.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>35635.0</td>\n",
       "      <td>293293.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>37543.8</td>\n",
       "      <td>233625.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>30825.2</td>\n",
       "      <td>227922.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>28298.2</td>\n",
       "      <td>246814.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>97899.4</td>\n",
       "      <td>471340.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>64710.4</td>\n",
       "      <td>373257.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>50966.2</td>\n",
       "      <td>343819.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>54870.4</td>\n",
       "      <td>304521.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>54821.6</td>\n",
       "      <td>327886.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>79226.0</td>\n",
       "      <td>233218.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>62504.0</td>\n",
       "      <td>480728.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>129507.8</td>\n",
       "      <td>260240.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>41799.6</td>\n",
       "      <td>326000.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>43280.4</td>\n",
       "      <td>249467.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65133.6</td>\n",
       "      <td>584654.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>67783.8</td>\n",
       "      <td>462165.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>111668.2</td>\n",
       "      <td>464377.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>84647.2</td>\n",
       "      <td>331799.6</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>96948.4</td>\n",
       "      <td>343388.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60045.6</td>\n",
       "      <td>327889.8</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>45442.2</td>\n",
       "      <td>284416.2</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60903.2</td>\n",
       "      <td>392232.0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>47928.8</td>\n",
       "      <td>151957.4</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gender  age  afftype  melanch  inpatient  edu  marriage  work  madrs1  \\\n",
       "0        2   37        2        2          2    8         1     2      19   \n",
       "1        2   42        1        2          2    8         2     2      24   \n",
       "2        1   47        2        2          2    8         2     2      24   \n",
       "3        2   27        2        2          2   13         1     1      20   \n",
       "4        2   52        2        2          2   13         2     2      26   \n",
       "5        1   37        2        2          2    8         1     2      18   \n",
       "6        1   22        1        2          2   13         2     1      24   \n",
       "7        2   27        2        2          2   13         1     2      20   \n",
       "8        2   47        1        2          2    8         1     2      26   \n",
       "9        2   47        2        2          2    8         1     2      28   \n",
       "10       1   47        2        2          2    8         1     2      24   \n",
       "11       2   42        1        2          2    8         2     2      25   \n",
       "12       2   37        1        2          2   13         2     2      18   \n",
       "13       1   62        1        2          2    8         2     2      28   \n",
       "14       2   57        2        2          2   13         1     1      14   \n",
       "15       1   47        2        2          2   13         1     2      13   \n",
       "16       1   52        1        2          2    8         1     2      17   \n",
       "17       2   42        3        2          2   13         2     2      18   \n",
       "18       2   52        2        2          1   18         2     2      26   \n",
       "19       1   32        2        1          1    8         1     2      27   \n",
       "20       2   37        2        2          1    8         2     2      26   \n",
       "21       1   67        2        2          1    8         2     2      29   \n",
       "22       1   32        2        2          1   18         2     2      29   \n",
       "23       2   27        0        2          0   13         2     1       4   \n",
       "24       1   32        0        2          0    8         1     1       4   \n",
       "25       2   32        0        2          0   18         1     1       4   \n",
       "26       1   27        0        2          0   18         2     2       4   \n",
       "27       1   32        0        2          0    8         1     1       4   \n",
       "28       1   27        0        2          0   13         2     1       9   \n",
       "29       1   22        0        2          0   13         2     1       4   \n",
       "30       2   42        0        2          0   18         1     1       4   \n",
       "31       2   32        0        2          0   18         1     1       4   \n",
       "32       1   32        0        2          0    8         1     1       4   \n",
       "33       1   47        0        2          0    8         1     1       4   \n",
       "34       1   62        0        2          0   13         1     1       4   \n",
       "35       1   52        0        2          0    8         1     1       4   \n",
       "36       1   52        0        2          0    8         1     1       4   \n",
       "37       1   47        0        2          0    8         1     1       4   \n",
       "38       2   42        0        2          0   18         1     1       4   \n",
       "39       1   47        0        2          0    8         1     1       4   \n",
       "40       2   22        0        2          0   13         2     1       4   \n",
       "41       1   52        0        2          0    8         1     1       4   \n",
       "42       1   37        0        2          0    8         1     1       9   \n",
       "43       1   52        0        2          0    8         1     1       4   \n",
       "44       1   27        0        2          0   18         1     1       4   \n",
       "45       1   22        0        2          0   13         2     1       4   \n",
       "46       2   22        0        2          0    8         2     1       4   \n",
       "47       1   67        0        2          0   13         1     1       4   \n",
       "48       1   37        0        2          0    8         1     1       4   \n",
       "49       2   52        0        2          0    8         2     1       4   \n",
       "50       2   47        0        2          0    8         2     1       4   \n",
       "51       2   52        0        2          0   13         2     1       4   \n",
       "52       2   37        0        2          0    8         1     1       4   \n",
       "53       1   22        2        2          0    8         2     1       4   \n",
       "54       2   27        0        2          0   13         2     1       4   \n",
       "\n",
       "    5days_sleep_time_activity  5days_day_time_activity         Id  \n",
       "0                     10693.6                 228824.0  condition  \n",
       "1                     48771.2                 239278.2  condition  \n",
       "2                     43211.0                 317726.0  condition  \n",
       "3                     56892.4                 194298.2  condition  \n",
       "4                     31303.8                 200302.2  condition  \n",
       "5                     26634.2                 240767.8  condition  \n",
       "6                     61643.6                 335598.2  condition  \n",
       "7                     34374.2                 284320.6  condition  \n",
       "8                     42992.0                 203120.0  condition  \n",
       "9                     25811.0                 482765.0  condition  \n",
       "10                    50147.0                 153494.6  condition  \n",
       "11                    34576.6                 228420.4  condition  \n",
       "12                    62330.8                 291661.6  condition  \n",
       "13                    42878.0                  72237.4  condition  \n",
       "14                    42016.2                 164387.4  condition  \n",
       "15                    17938.8                 397321.0  condition  \n",
       "16                    24344.6                  85835.4  condition  \n",
       "17                    40038.4                  64142.0  condition  \n",
       "18                    17014.0                 189703.4  condition  \n",
       "19                    10301.8                  84496.8  condition  \n",
       "20                     4627.0                  91568.8  condition  \n",
       "21                    18023.6                 183974.4  condition  \n",
       "22                    29581.2                 235831.4  condition  \n",
       "23                    62703.4                 257546.4    control  \n",
       "24                    51929.0                 475113.4    control  \n",
       "25                    34742.8                 347817.0    control  \n",
       "26                    23481.0                 255676.4    control  \n",
       "27                    49904.8                 398723.0    control  \n",
       "28                   103205.0                 431101.8    control  \n",
       "29                   115329.4                 479807.0    control  \n",
       "30                   124525.0                 467588.6    control  \n",
       "31                    19149.8                 169119.2    control  \n",
       "32                    35635.0                 293293.2    control  \n",
       "33                    37543.8                 233625.8    control  \n",
       "34                    30825.2                 227922.0    control  \n",
       "35                    28298.2                 246814.8    control  \n",
       "36                    97899.4                 471340.6    control  \n",
       "37                    64710.4                 373257.6    control  \n",
       "38                    50966.2                 343819.2    control  \n",
       "39                    54870.4                 304521.0    control  \n",
       "40                    54821.6                 327886.0    control  \n",
       "41                    79226.0                 233218.6    control  \n",
       "42                    62504.0                 480728.2    control  \n",
       "43                   129507.8                 260240.4    control  \n",
       "44                    41799.6                 326000.6    control  \n",
       "45                    43280.4                 249467.8    control  \n",
       "46                    65133.6                 584654.0    control  \n",
       "47                    67783.8                 462165.4    control  \n",
       "48                   111668.2                 464377.6    control  \n",
       "49                    84647.2                 331799.6    control  \n",
       "50                    96948.4                 343388.2    control  \n",
       "51                    60045.6                 327889.8    control  \n",
       "52                    45442.2                 284416.2    control  \n",
       "53                    60903.2                 392232.0    control  \n",
       "54                    47928.8                 151957.4    control  "
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df = pd.read_csv(\n",
    "    \"../Dataset/dataset_filled_missing.csv\")\n",
    "ml_df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "ml_df['Id'] = ['condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'condition', 'control', 'control',\n",
    "            'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control', 'control']\n",
    "ml_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select: 55 set\n",
      "[[1.069360e+04 2.288240e+05 3.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.900000e+01\n",
      "  1.000000e+00]\n",
      " [4.877120e+04 2.392782e+05 4.200000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [4.321100e+04 3.177260e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [5.689240e+04 1.942982e+05 2.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 2.000000e+01\n",
      "  1.000000e+00]\n",
      " [3.130380e+04 2.003022e+05 5.200000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [2.663420e+04 2.407678e+05 3.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.800000e+01\n",
      "  1.000000e+00]\n",
      " [6.164360e+04 3.355982e+05 2.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 2.400000e+01\n",
      "  2.000000e+00]\n",
      " [3.437420e+04 2.843206e+05 2.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 2.000000e+01\n",
      "  1.000000e+00]\n",
      " [4.299200e+04 2.031200e+05 4.700000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.600000e+01\n",
      "  1.000000e+00]\n",
      " [2.581100e+04 4.827650e+05 4.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.800000e+01\n",
      "  1.000000e+00]\n",
      " [5.014700e+04 1.534946e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.400000e+01\n",
      "  1.000000e+00]\n",
      " [3.457660e+04 2.284204e+05 4.200000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.500000e+01\n",
      "  2.000000e+00]\n",
      " [6.233080e+04 2.916616e+05 3.700000e+01 2.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.800000e+01\n",
      "  2.000000e+00]\n",
      " [4.287800e+04 7.223740e+04 6.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 2.800000e+01\n",
      "  2.000000e+00]\n",
      " [4.201620e+04 1.643874e+05 5.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 1.000000e+00 1.400000e+01\n",
      "  1.000000e+00]\n",
      " [1.793880e+04 3.973210e+05 4.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.300000e+01\n",
      "  1.000000e+00]\n",
      " [2.434460e+04 8.583540e+04 5.200000e+01 1.000000e+00 1.000000e+00\n",
      "  2.000000e+00 2.000000e+00 8.000000e+00 2.000000e+00 1.700000e+01\n",
      "  1.000000e+00]\n",
      " [4.003840e+04 6.414200e+04 4.200000e+01 2.000000e+00 3.000000e+00\n",
      "  2.000000e+00 2.000000e+00 1.300000e+01 2.000000e+00 1.800000e+01\n",
      "  2.000000e+00]\n",
      " [1.701400e+04 1.897034e+05 5.200000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 1.800000e+01 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [1.030180e+04 8.449680e+04 3.200000e+01 1.000000e+00 2.000000e+00\n",
      "  1.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.700000e+01\n",
      "  1.000000e+00]\n",
      " [4.627000e+03 9.156880e+04 3.700000e+01 2.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.600000e+01\n",
      "  2.000000e+00]\n",
      " [1.802360e+04 1.839744e+05 6.700000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 8.000000e+00 2.000000e+00 2.900000e+01\n",
      "  2.000000e+00]\n",
      " [2.958120e+04 2.358314e+05 3.200000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 1.000000e+00 1.800000e+01 2.000000e+00 2.900000e+01\n",
      "  2.000000e+00]\n",
      " [6.270340e+04 2.575464e+05 2.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [5.192900e+04 4.751134e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.474280e+04 3.478170e+05 3.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.348100e+04 2.556764e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 2.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [4.990480e+04 3.987230e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.032050e+05 4.311018e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 9.000000e+00\n",
      "  2.000000e+00]\n",
      " [1.153294e+05 4.798070e+05 2.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [1.245250e+05 4.675886e+05 4.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.914980e+04 1.691192e+05 3.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.563500e+04 2.932932e+05 3.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.754380e+04 2.336258e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [3.082520e+04 2.279220e+05 6.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [2.829820e+04 2.468148e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [9.789940e+04 4.713406e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.471040e+04 3.732576e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [5.096620e+04 3.438192e+05 4.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [5.487040e+04 3.045210e+05 4.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [5.482160e+04 3.278860e+05 2.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [7.922600e+04 2.332186e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.250400e+04 4.807282e+05 3.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 9.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.295078e+05 2.602404e+05 5.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.179960e+04 3.260006e+05 2.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.800000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [4.328040e+04 2.494678e+05 2.200000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [6.513360e+04 5.846540e+05 2.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [6.778380e+04 4.621654e+05 6.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [1.116682e+05 4.643776e+05 3.700000e+01 1.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [8.464720e+04 3.317996e+05 5.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [9.694840e+04 3.433882e+05 4.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [6.004560e+04 3.278898e+05 5.200000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [4.544220e+04 2.844162e+05 3.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  1.000000e+00]\n",
      " [6.090320e+04 3.922320e+05 2.200000e+01 1.000000e+00 2.000000e+00\n",
      "  2.000000e+00 0.000000e+00 8.000000e+00 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]\n",
      " [4.792880e+04 1.519574e+05 2.700000e+01 2.000000e+00 0.000000e+00\n",
      "  2.000000e+00 0.000000e+00 1.300000e+01 1.000000e+00 4.000000e+00\n",
      "  2.000000e+00]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "x = [] \n",
    "y = [] \n",
    "count  = 0\n",
    "count_g = 0\n",
    "count_o = 0\n",
    "\n",
    "for i in range(len(ml_df[\"Id\"])):\n",
    "    \n",
    "    if ml_df[\"Id\"][i] == \"condition\":\n",
    "        x.append(1)\n",
    "        y.append([ml_df[\"5days_sleep_time_activity\"][i],\n",
    "                  ml_df[\"5days_day_time_activity\"][i], ml_df[\"age\"][i], ml_df[\"gender\"][i], ml_df[\"afftype\"][i], \n",
    "                  ml_df[\"melanch\"][i], ml_df[\"inpatient\"][i], ml_df[\"edu\"][i], ml_df[\"work\"][i], ml_df[\"madrs1\"][i], ml_df[\"marriage\"][i]])\n",
    "                           \n",
    "    \n",
    "    elif ml_df[\"Id\"][i] == \"control\":\n",
    "        x.append(0)\n",
    "        y.append([ml_df[\"5days_sleep_time_activity\"][i],\n",
    "                  ml_df[\"5days_day_time_activity\"][i], ml_df[\"age\"][i], ml_df[\"gender\"][i], ml_df[\"afftype\"][i],\n",
    "                  ml_df[\"melanch\"][i], ml_df[\"inpatient\"][i], ml_df[\"edu\"][i], ml_df[\"work\"][i], ml_df[\"madrs1\"][i], ml_df[\"marriage\"][i]])\n",
    "    \n",
    "\n",
    "    \n",
    "print(f'Select: {len(y)} set')\n",
    "y = np.array(y)\n",
    "x = np.array(x)\n",
    "\n",
    "print(y)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "# Train Test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "history = []\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_modeling():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim = 11, activation = 'relu'))\n",
    "    model.add(Dense(2, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(y_train, x_train, epochs=50, batch_size=10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 1ms/step - loss: 75696.2812 - accuracy: 0.4545\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 73856.9062 - accuracy: 0.4545\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 72059.4453 - accuracy: 0.4545\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 70406.8672 - accuracy: 0.4545\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 68557.1172 - accuracy: 0.4545\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 66940.1797 - accuracy: 0.4545\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 65244.7109 - accuracy: 0.4545\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 63620.7148 - accuracy: 0.4545\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 61966.1484 - accuracy: 0.4545\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 60456.5625 - accuracy: 0.4545\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 58817.0234 - accuracy: 0.4545\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 57367.9727 - accuracy: 0.4545\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 55789.2891 - accuracy: 0.4545\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 54346.9766 - accuracy: 0.4545\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 52825.7109 - accuracy: 0.4545\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 51398.6836 - accuracy: 0.4545\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 50025.0352 - accuracy: 0.4545\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 48612.6367 - accuracy: 0.4545\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 47405.2539 - accuracy: 0.4545\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 46020.4766 - accuracy: 0.4545\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 44768.5039 - accuracy: 0.4545\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 43577.2656 - accuracy: 0.4545\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 42437.5234 - accuracy: 0.4545\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 41202.0273 - accuracy: 0.4545\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 40193.7773 - accuracy: 0.4545\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 38924.8789 - accuracy: 0.4545\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 37914.0352 - accuracy: 0.4545\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 36818.0078 - accuracy: 0.4545\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 35809.3477 - accuracy: 0.4545\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 34736.8164 - accuracy: 0.4545\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 33735.6250 - accuracy: 0.4545\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 32772.4492 - accuracy: 0.4545\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 31708.3926 - accuracy: 0.4545\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 30882.6641 - accuracy: 0.4545\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 29893.5566 - accuracy: 0.4545\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 29031.9805 - accuracy: 0.4545\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28086.8301 - accuracy: 0.4545\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 27243.5176 - accuracy: 0.4545\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 26374.4863 - accuracy: 0.4545\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 25567.3359 - accuracy: 0.4545\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 24806.4922 - accuracy: 0.4545\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 23936.7832 - accuracy: 0.4545\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 23077.0078 - accuracy: 0.4545\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 22328.1953 - accuracy: 0.4545\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 21530.3730 - accuracy: 0.4545\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 20780.4688 - accuracy: 0.4545\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 20033.5723 - accuracy: 0.4545\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 19259.3516 - accuracy: 0.4545\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 18552.0352 - accuracy: 0.4545\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 17805.2207 - accuracy: 0.4545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1e266d027c0>"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_modeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 209ms/step - loss: 15288.8848 - accuracy: 0.7273\n",
      "Accuracy: 72.73\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(y_test, x_test)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(y)\n",
    "rounded = [round(y[0]) for y in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 20154.1953 - accuracy: 0.5455 - val_loss: 13906.9756 - val_accuracy: 0.7273\n",
      "Epoch 2/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18361.1973 - accuracy: 0.5909 - val_loss: 12584.0713 - val_accuracy: 0.7273\n",
      "Epoch 3/30\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16661.1504 - accuracy: 0.5909 - val_loss: 11449.0635 - val_accuracy: 0.7273\n",
      "Epoch 4/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15164.6465 - accuracy: 0.5909 - val_loss: 10340.8389 - val_accuracy: 0.6364\n",
      "Epoch 5/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13681.4053 - accuracy: 0.5909 - val_loss: 9378.9893 - val_accuracy: 0.6364\n",
      "Epoch 6/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12363.4521 - accuracy: 0.5909 - val_loss: 8467.3447 - val_accuracy: 0.6364\n",
      "Epoch 7/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11182.9355 - accuracy: 0.5909 - val_loss: 7537.5229 - val_accuracy: 0.6364\n",
      "Epoch 8/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9996.6279 - accuracy: 0.5909 - val_loss: 6696.7598 - val_accuracy: 0.6364\n",
      "Epoch 9/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8886.4150 - accuracy: 0.5909 - val_loss: 5916.9424 - val_accuracy: 0.6364\n",
      "Epoch 10/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7803.4683 - accuracy: 0.5682 - val_loss: 5238.6411 - val_accuracy: 0.6364\n",
      "Epoch 11/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6905.4067 - accuracy: 0.5909 - val_loss: 4537.2568 - val_accuracy: 0.6364\n",
      "Epoch 12/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 6017.5420 - accuracy: 0.5909 - val_loss: 3944.6726 - val_accuracy: 0.6364\n",
      "Epoch 13/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 5262.1353 - accuracy: 0.5909 - val_loss: 3436.4282 - val_accuracy: 0.5455\n",
      "Epoch 14/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4650.6553 - accuracy: 0.5909 - val_loss: 2826.4341 - val_accuracy: 0.5455\n",
      "Epoch 15/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 4012.8779 - accuracy: 0.5000 - val_loss: 2289.9373 - val_accuracy: 0.5455\n",
      "Epoch 16/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 3439.2227 - accuracy: 0.5000 - val_loss: 1765.9500 - val_accuracy: 0.5455\n",
      "Epoch 17/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2935.1790 - accuracy: 0.5455 - val_loss: 1374.8524 - val_accuracy: 0.5455\n",
      "Epoch 18/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2587.0322 - accuracy: 0.5682 - val_loss: 1020.6500 - val_accuracy: 0.5455\n",
      "Epoch 19/30\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 2318.0535 - accuracy: 0.5227 - val_loss: 685.7133 - val_accuracy: 0.4545\n",
      "Epoch 20/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 2038.8081 - accuracy: 0.5000 - val_loss: 409.0731 - val_accuracy: 0.2727\n",
      "Epoch 21/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1794.7289 - accuracy: 0.4773 - val_loss: 160.9301 - val_accuracy: 0.4545\n",
      "Epoch 22/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1531.9734 - accuracy: 0.4091 - val_loss: 23.4594 - val_accuracy: 0.4545\n",
      "Epoch 23/30\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 1263.3896 - accuracy: 0.3864 - val_loss: 0.5826 - val_accuracy: 0.5455\n",
      "Epoch 24/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 1045.8922 - accuracy: 0.3409 - val_loss: 0.6755 - val_accuracy: 0.4545\n",
      "Epoch 25/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 845.1078 - accuracy: 0.3636 - val_loss: 0.7673 - val_accuracy: 0.3636\n",
      "Epoch 26/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 706.0947 - accuracy: 0.3409 - val_loss: 0.7659 - val_accuracy: 0.3636\n",
      "Epoch 27/30\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 626.4773 - accuracy: 0.3409 - val_loss: 0.7649 - val_accuracy: 0.3636\n",
      "Epoch 28/30\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 512.0901 - accuracy: 0.3409 - val_loss: 0.8545 - val_accuracy: 0.2727\n",
      "Epoch 29/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 445.9252 - accuracy: 0.3636 - val_loss: 0.8521 - val_accuracy: 0.2727\n",
      "Epoch 30/30\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 384.5642 - accuracy: 0.3636 - val_loss: 0.8505 - val_accuracy: 0.2727\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "history = model.fit(y_train, x_train, validation_data=(\n",
    "    y_test, x_test), epochs=30, batch_size=2, verbose=1)\n",
    "\n",
    "x_pred = model.predict(y_test).ravel()\n",
    "\n",
    "nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(x_test, x_pred)\n",
    "auc_keras = auc(nn_fpr_keras, nn_tpr_keras)\n",
    "plt.plot(nn_fpr_keras, nn_tpr_keras, marker='.',\n",
    "         label='Neural Network (auc = %0.3f)' % auc_keras)\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 2ms/step - loss: 0.6932 - accuracy: 0.4545\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5455\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5455\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5455\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.5455\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6929 - accuracy: 0.5455\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6929 - accuracy: 0.5455\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6928 - accuracy: 0.5455\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6927 - accuracy: 0.5455\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6927 - accuracy: 0.5455\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5455\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6926 - accuracy: 0.5455\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5455\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5455\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.5455\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5455\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6922 - accuracy: 0.5455\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6923 - accuracy: 0.5455\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5455\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5455\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5455\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5455\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5455\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5455\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5455\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5455\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5455\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5455\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5455\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.5455\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6920 - accuracy: 0.5455\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5455\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5455\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5455\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5455\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6918 - accuracy: 0.5455\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.5455\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5455\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5455\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6916 - accuracy: 0.5455\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6915 - accuracy: 0.5455\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5455\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6915 - accuracy: 0.5455\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.5455\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5455\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5455\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6914 - accuracy: 0.5455\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5455\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6913 - accuracy: 0.5455\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6913 - accuracy: 0.5455\n",
      "R :  1\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 2)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\CN thammasart\\CN22021\\CN240\\CN240-ML\\Deep_Learning\\ANN_filled.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000021?line=27'>28</a>\u001b[0m y_val \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_val)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000021?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mR : \u001b[39m\u001b[39m\"\u001b[39m, i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000021?line=29'>30</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_plot_train, y_plot_train, validation_data\u001b[39m=\u001b[39;49m(X_val, y_val), epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000021?line=31'>32</a>\u001b[0m value \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_plot[test])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000021?line=32'>33</a>\u001b[0m y_true \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(y_plot_test,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/safec/AppData/Local/Packages/PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0/LocalCache/local-packages/Python38/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"C:\\Users\\safec\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 2)).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits = 5)\n",
    "X_plot = y_train\n",
    "y_plot = x_train\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "X_plot_train_list = []\n",
    "y_plot_train_list = []\n",
    "\n",
    "X_plot_val_list = []\n",
    "y_plot_val_list = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for i, (train, test) in enumerate(kf.split(X_plot, y_plot)):\n",
    "    model = classifier_modeling()\n",
    "\n",
    "    X_plot_train, X_val, y_plot_train, y_val = train_test_split(X_plot[train], y_plot[train], test_size=0.1, random_state=15, stratify=y_plot[train])\n",
    "\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_plot_train)\n",
    "    y_plot_train = le.transform(y_plot_train)\n",
    "    y_plot_test = le.transform(y_plot[test])\n",
    "    y_val = le.transform(y_val)\n",
    "    y_pot_train = keras.utils.to_categorical(y_plot_train)\n",
    "    y_plot_test = keras.utils.to_categorical(y_plot_test)\n",
    "    y_val = keras.utils.to_categorical(y_val)\n",
    "    print(\"R : \", i + 1)\n",
    "    model.fit(X_plot_train, y_plot_train, validation_data=(X_val, y_val), epochs=100, verbose=2)\n",
    "\n",
    "    value = model.predict(X_plot[test])\n",
    "    y_true = np.argmax(y_plot_test,axis=1)\n",
    "\n",
    "    viz = RocCurveDisplay.from_predictions(y_true, value[:,1],name=\"ROC fold {}\".format(i + 1),alpha=0.5,lw=1,ax=ax)\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "    X_plot_train_list.append(X_plot_train)\n",
    "    y_plot_train_list.append(y_plot_train)\n",
    "    \n",
    "    X_plot_val_list.append(X_val)\n",
    "    y_plot_val_list.append(y_val)\n",
    "    \n",
    "\n",
    "    \n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"r\", alpha=0.8)\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color=\"b\", label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc), lw=2, alpha=0.8,)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color=\"grey\", alpha=0.2, label=r\"$\\pm$ 1 std. dev.\",)\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05], title=\"Receiver operating characteristic NN(elu)\",)\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 583.9760 - accuracy: 0.5455\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 499.2799 - accuracy: 0.5455\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 426.6746 - accuracy: 0.5455\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 350.0535 - accuracy: 0.5455\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 285.7427 - accuracy: 0.5455\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 219.0531 - accuracy: 0.5455\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 164.7598 - accuracy: 0.5455\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 108.9001 - accuracy: 0.5455\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 66.7596 - accuracy: 0.5455\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 21.6504 - accuracy: 0.5455\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 17.5612 - accuracy: 0.4545\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 10.8255 - accuracy: 0.4545\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 12.9672 - accuracy: 0.5455\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 13.6909 - accuracy: 0.5455\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 4.4641 - accuracy: 0.5909\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 8.2860 - accuracy: 0.3182\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 5.7680 - accuracy: 0.5455\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 3.2721 - accuracy: 0.3636\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 4.0247 - accuracy: 0.5455\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.5743 - accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.8056 - accuracy: 0.5455\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.2215 - accuracy: 0.4545\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.8502 - accuracy: 0.4545\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0724 - accuracy: 0.3636\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.5132 - accuracy: 0.5455\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.9543 - accuracy: 0.5455\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.5101 - accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.1827 - accuracy: 0.4545\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.7124 - accuracy: 0.5682\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.5573 - accuracy: 0.4091\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0606 - accuracy: 0.5455\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.5796 - accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.9783 - accuracy: 0.5227\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8610 - accuracy: 0.4318\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.9633 - accuracy: 0.5455\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.3578 - accuracy: 0.5682\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.9322 - accuracy: 0.6364\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.5344 - accuracy: 0.5455\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.3502 - accuracy: 0.5455\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0587 - accuracy: 0.4545\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6555 - accuracy: 0.6591\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.0628 - accuracy: 0.5909\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7626 - accuracy: 0.5455\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.4365 - accuracy: 0.5455\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.5041 - accuracy: 0.5455\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.1428 - accuracy: 0.3636\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.2083 - accuracy: 0.4545\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 0.8333 - accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 1.0225 - accuracy: 0.5455\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.8479 - accuracy: 0.6364\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\CN thammasart\\CN22021\\CN240\\CN240-ML\\Deep_Learning\\ANN_filled.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000020?line=32'>33</a>\u001b[0m y_plot_test \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mtransform(y_plot[test])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000020?line=33'>34</a>\u001b[0m y_val \u001b[39m=\u001b[39m le\u001b[39m.\u001b[39mtransform(y_val)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000020?line=34'>35</a>\u001b[0m y_plot_train \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39mto_categorical(y_plot_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000020?line=35'>36</a>\u001b[0m y_plot_test \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_plot_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CN%20thammasart/CN22021/CN240/CN240-ML/Deep_Learning/ANN_filled.ipynb#ch0000020?line=36'>37</a>\u001b[0m y_val \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mto_categorical(y_val)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'utils'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "\n",
    "# # Train Test split\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     x, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "x_plot = y_train\n",
    "y_plot = x_train\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "x_plot_train_list = []\n",
    "y_plot_train_list = []\n",
    "\n",
    "x_plot_val_list = []\n",
    "y_plot_val_train = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "for i, (train, test) in enumerate(cv.split(x_plot, y_plot)):\n",
    "    model = classifier_modeling()\n",
    "\n",
    "    x_plot_train, x_val, y_plot_train, y_val = train_test_split(\n",
    "        x_plot[train], y_plot[train], test_size=0.1, random_state=15, stratify=y_plot[train])\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(y_plot_train)\n",
    "    y_plot_train = le.transform(y_plot_train)\n",
    "    y_plot_test = le.transform(y_plot[test])\n",
    "    y_val = le.transform(y_val)\n",
    "    y_plot_train = tf.utils.to_categorical(y_plot_train)\n",
    "    y_plot_test = tf.utils.to_categorical(y_plot_test)\n",
    "    y_val = tf.utils.to_categorical(y_val)\n",
    "    print(\"R : \", i + 1)\n",
    "    model.fit(x_plot_train, y_plot_train, validation_data=(x_val, y_val), epochs=50, batch_size=10)\n",
    "\n",
    "    value = model.predict(x_plot[test])\n",
    "    y_true = np.argmax(y_plot_test, axis=1)\n",
    "\n",
    "    viz = RocCurveDisplay.from_predictions(y_true, value[:, 1], name=\"ROC fold {}\".format(i + 1),\n",
    "                                           alpha=0.3, lw=1, ax=ax)\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "    x_plot_train_list.append(x_plot.train)\n",
    "    y_plot_train_list.append(y_plot.train)\n",
    "\n",
    "    x_plot_val_list.append(x_val)\n",
    "    y_plot_val_list.append(y_val)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae305f4dee197bdc0c916b004f5792eca0da2328f3eb5fe8e31b308819fc7eb2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
